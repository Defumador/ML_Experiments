{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ski lean ml classifier naive bayes random forest svc  sgd",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manishiitg/ML_Experments/blob/master/ml/ski_lean_ml_classifier_naive_bayes_random_forest_svc_sgd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjNasmYMl61R",
        "colab_type": "text"
      },
      "source": [
        "Trying to different ML classifiers like naive bayes, random forest, svc, sgd for multi label classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRPGb84pwfln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import spacy \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from sklearn.cluster import KMeans;\n",
        "from sklearn.neighbors import KDTree;\n",
        "\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt;\n",
        "from itertools import cycle;\n",
        "import matplotlib.pyplot as plt;\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "from collections import defaultdict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMgkCpDQwiDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(comment, lowercase=True, remove_stopwords=True):\n",
        "    if lowercase:\n",
        "        comment = comment.lower()\n",
        "    lines = comment.splitlines()\n",
        "    lines = [x.strip(' ') for x in lines]\n",
        "    lines = [x.replace('\"', '') for x in lines]\n",
        "    lines = [x.replace('\\\\\"', '') for x in lines]\n",
        "    lines = [x.replace(u'\\xa0', u'') for x in lines]\n",
        "    comment = \" \".join(lines)\n",
        "    doc = nlp(comment)\n",
        "\n",
        "    # for token in doc:\n",
        "    #   print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "    #     token.shape_, token.is_alpha, token.is_stop)\n",
        "\n",
        "    words = [token for token in doc if token.is_stop !=\n",
        "             True and token.is_punct != True]\n",
        "    # return \" \".join(words)\n",
        "    lemmatized = list()\n",
        "    for word in words:\n",
        "        lemma = word.lemma_.strip()\n",
        "        if lemma:\n",
        "            lemmatized.append(lemma)\n",
        "    return lemmatized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXOHKviUwjkS",
        "colab_type": "code",
        "outputId": "7be4507a-4f8a-4b10-f3b2-e63205978553",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "import os\n",
        "\n",
        "def writetofile(dir, filename, data):\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir)\n",
        "    f = os.path.join(dir,str(filename))\n",
        "\n",
        "    with open(f, 'wb') as the_file:\n",
        "      the_file.write(data)\n",
        "\n",
        "news = fetch_20newsgroups(subset=\"train\", categories=[ 'comp.graphics','comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware','comp.sys.mac.hardware','comp.windows.x'])\n",
        "\n",
        "\n",
        "clean_data  = []\n",
        "\n",
        "# print(news.keys())\n",
        "\n",
        "# print(news[\"filenames\"][:10])\n",
        "\n",
        "# print(len(news[\"data\"][:1000]))\n",
        "\n",
        "# print(news[\"target_names\"][:50])\n",
        "# print(news[\"target\"][:10])\n",
        "\n",
        "\n",
        "max_limit = 4000\n",
        "\n",
        "targets = news[\"target\"][:max_limit]\n",
        "filenames = news[\"filenames\"][:max_limit]\n",
        "news = news[\"data\"][:max_limit]\n",
        "\n",
        "dir = \"news_group_cleaned\"\n",
        "\n",
        "print(\"cleaning data\")\n",
        "for i, row in enumerate(news):\n",
        "  filename = filenames[i]\n",
        "  filename = filename[(filename.rfind('/'))+1:]\n",
        "  if os.path.exists(filename):\n",
        "    with open(os.path.join(dir, filename), 'r') as content_file:\n",
        "      data = content_file.read()\n",
        "      cleaned = data.split(\" \")\n",
        "  else:\n",
        "    cleaned = normalize(row)\n",
        "    writetofile(dir, filename, \" \".join(cleaned).encode(\"utf-8\"))\n",
        "    \n",
        "  \n",
        "  clean_data.append(cleaned)\n",
        "\n",
        "print(\"data cleaned\")\n",
        "\n",
        "# print(clean_data[10])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cleaning data\n",
            "data cleaned\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYELJcUmwk_r",
        "colab_type": "code",
        "outputId": "7b64c68b-cdbf-4c0f-c79b-f8336c81a503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "source": [
        "import gensim, logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "model = gensim.models.Word2Vec(clean_data)\n",
        "\n",
        "\n",
        "# model = KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec')\n",
        "\n",
        "#https://stackoverflow.com/questions/45159693/word2vec-models-consist-of-characters-instead-of-words"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-11-08 06:06:24,021 : INFO : collecting all words and their counts\n",
            "2019-11-08 06:06:24,022 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2019-11-08 06:06:24,140 : INFO : collected 66979 word types from a corpus of 470869 raw words and 2936 sentences\n",
            "2019-11-08 06:06:24,141 : INFO : Loading a fresh vocabulary\n",
            "2019-11-08 06:06:24,179 : INFO : effective_min_count=5 retains 7758 unique words (11% of original 66979, drops 59221)\n",
            "2019-11-08 06:06:24,180 : INFO : effective_min_count=5 leaves 392121 word corpus (83% of original 470869, drops 78748)\n",
            "2019-11-08 06:06:24,207 : INFO : deleting the raw counts dictionary of 66979 items\n",
            "2019-11-08 06:06:24,211 : INFO : sample=0.001 downsamples 33 most-common words\n",
            "2019-11-08 06:06:24,212 : INFO : downsampling leaves estimated 326228 word corpus (83.2% of prior 392121)\n",
            "2019-11-08 06:06:24,237 : INFO : estimated required memory for 7758 words and 100 dimensions: 10085400 bytes\n",
            "2019-11-08 06:06:24,238 : INFO : resetting layer weights\n",
            "2019-11-08 06:06:25,668 : INFO : training model with 3 workers on 7758 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2019-11-08 06:06:26,268 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-08 06:06:26,283 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-08 06:06:26,298 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-08 06:06:26,299 : INFO : EPOCH - 1 : training on 470869 raw words (326095 effective words) took 0.6s, 520929 effective words/s\n",
            "2019-11-08 06:06:26,906 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-08 06:06:26,912 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-08 06:06:26,917 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-08 06:06:26,918 : INFO : EPOCH - 2 : training on 470869 raw words (326215 effective words) took 0.6s, 533808 effective words/s\n",
            "2019-11-08 06:06:27,493 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-08 06:06:27,508 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-08 06:06:27,521 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-08 06:06:27,522 : INFO : EPOCH - 3 : training on 470869 raw words (325928 effective words) took 0.6s, 542952 effective words/s\n",
            "2019-11-08 06:06:28,101 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-08 06:06:28,116 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-08 06:06:28,126 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-08 06:06:28,127 : INFO : EPOCH - 4 : training on 470869 raw words (326149 effective words) took 0.6s, 543761 effective words/s\n",
            "2019-11-08 06:06:28,713 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-08 06:06:28,727 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-08 06:06:28,739 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-08 06:06:28,740 : INFO : EPOCH - 5 : training on 470869 raw words (326035 effective words) took 0.6s, 534459 effective words/s\n",
            "2019-11-08 06:06:28,741 : INFO : training on a 2354345 raw words (1630422 effective words) took 3.1s, 530770 effective words/s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t6jF0ngwoR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TfidfEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        self.word2weight = None\n",
        "        self.dim = 100\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
        "        tfidf.fit(X)\n",
        "        # if a word was never seen - it must be at least as infrequent\n",
        "        # as any of the known words - so the default idf is the max of \n",
        "        # known idf's\n",
        "        max_idf = max(tfidf.idf_)\n",
        "        #https://stackoverflow.com/a/5900634\n",
        "        self.word2weight = defaultdict(\n",
        "            lambda: max_idf,\n",
        "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
        "                         for w in words if w in self.word2vec] or\n",
        "                        [np.zeros(self.dim)], axis=0)\n",
        "                for words in X\n",
        "            ])\n",
        "\n",
        "w2v = dict(zip(model.wv.index2word, model.wv.vectors))\n",
        "\n",
        "tf = TfidfEmbeddingVectorizer(w2v)\n",
        "tf.fit(clean_data, targets)\n",
        "embeddings = tf.transform(clean_data)   \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aAbMz25wpmg",
        "colab_type": "code",
        "outputId": "60d9d340-364d-4845-b27f-7324a6895021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(len(embeddings))\n",
        "print(len(targets))\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(embeddings, targets, test_size=0.33, random_state=42)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2936\n",
            "2936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Og8nf0CSzCI0",
        "colab_type": "code",
        "outputId": "c9a51607-80b8-41e5-fec0-3cf911385279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "clf = GaussianNB()\n",
        "clf.fit(X_train, Y_train)\n",
        "\n",
        "pred = clf.predict(X_test)\n",
        "\n",
        "clf.score(X_test, Y_test)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4282765737874097"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrCWCLE_9zkY",
        "colab_type": "code",
        "outputId": "b4eae115-564a-44f6-9e1e-fc55f8139e9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
        "clf.fit(X_train, Y_train) \n",
        "\n",
        "pred = clf.predict(X_test)\n",
        "\n",
        "clf.score(X_test, Y_test)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4674922600619195"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qx9z0ey95mJ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i72Q0QZU3FQ9",
        "colab_type": "code",
        "outputId": "869c4e9f-7dce-4d16-82b1-3639b82f4eb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, Y_train)\n",
        "\n",
        "pred = clf.predict(X_test)\n",
        "\n",
        "clf.score(X_test, Y_test)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-3c7f775f8014>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    611\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[1;32m    612\u001b[0m                                        dtype=np.float64)\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input X must be non-negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input X must be non-negative"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xXj0VPpSY96",
        "colab_type": "code",
        "outputId": "f7929c90-efe8-40d4-e4a4-5c9da8ff4735",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(X_train, Y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "clf.score(X_test, Y_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6749226006191951"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5ELAHIg9wuJ",
        "colab_type": "code",
        "outputId": "562817b2-5632-4f1d-8d08-9acf049f63c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
        "gs_clf = GridSearchCV(clf, parameters, cv=5, iid=False, n_jobs=-1)\n",
        "\n",
        "gs_clf = gs_clf.fit(X_train[:400], Y_train[:400])\n",
        "\n",
        "print(gs_clf.best_score_)\n",
        "print(gs_clf.best_params_)\n",
        "\n",
        "for param_name in sorted(parameters.keys()):\n",
        "  print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6075988582291968\n",
            "{'C': 10, 'kernel': 'linear'}\n",
            "C: 10\n",
            "kernel: 'linear'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT093Gyki-fU",
        "colab_type": "code",
        "outputId": "03c7635d-dd03-487e-fc5e-ee72aa8d9987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# clf = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
        "clf = SGDClassifier()\n",
        "clf.fit(X_train, Y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "clf.score(X_test, Y_test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6398348813209495"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-iZBaZ4jmCU",
        "colab_type": "text"
      },
      "source": [
        "more important we can see how tune parameters one a model is chosen using\n",
        "\n",
        "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#parameter-tuning-using-grid-search\n",
        "\n",
        "also look at classification report \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_D3Wk9AkU7n",
        "colab_type": "text"
      },
      "source": [
        "OneVsRest strategy can be used for multi-label learning, where a classifier is used to predict multiple labels for instance. Naive Bayes supports multi-class, but we are in a multi-label scenario, therefore, we wrap Naive Bayes in the OneVsRestClassifier."
      ]
    }
  ]
}