{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf idf experments",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manishiitg/ML_Experments/blob/master/nlp/101/tf_idf_experments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r-mo80RscTM",
        "colab_type": "text"
      },
      "source": [
        "**TFIDF**\n",
        "\n",
        "Term Frequency - Inverse Document Frequency \n",
        "\n",
        "Implement and try out TFIDF using sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DR2Vvd66eHW",
        "colab_type": "code",
        "outputId": "3623ea79-a51d-4d2c-b754-6640abb07990",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import spacy \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVHFSK8xEd6s",
        "colab_type": "code",
        "outputId": "a8a20295-cbee-4405-e776-f99b4b60f446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "docs= [\n",
        "  \"NLP is awesome\",\n",
        "  \"I love NLP\"\n",
        "]\n",
        "\n",
        "\n",
        "count_vectorizer = TfidfVectorizer(\n",
        "    analyzer=\"word\", tokenizer=nltk.word_tokenize,\n",
        "    preprocessor=None, stop_words='english', max_features=None)    \n",
        "\n",
        "tfidf = count_vectorizer.fit_transform(docs)\n",
        "\n",
        "print(tfidf.todense())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.81480247 0.         0.57973867]\n",
            " [0.         0.81480247 0.57973867]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl1KclJzstRY",
        "colab_type": "text"
      },
      "source": [
        "Just to understand how the outputs would be. create a very simple array and see the output.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60ka2fyss2ld",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ab18792c-62f3-4cb1-cd28-4b6f0f50961e"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "docs= [\n",
        "  \"NLP is awesome\",\n",
        "  \"I love NLP\"\n",
        "]\n",
        "\n",
        "\n",
        "count_vectorizer = CountVectorizer(\n",
        "    analyzer=\"word\", tokenizer=nltk.word_tokenize,\n",
        "    preprocessor=None, stop_words='english', max_features=None)    \n",
        "\n",
        "bag_of_words = count_vectorizer.fit_transform(docs)\n",
        "\n",
        "print(bag_of_words.todense())\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 1]\n",
            " [0 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAf__X8btDsI",
        "colab_type": "text"
      },
      "source": [
        "Comparing TF-IDF to Bag of words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOjgzdFPJI1M",
        "colab_type": "code",
        "outputId": "10056a3f-4ced-4e04-8a0e-574196b155bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "news = fetch_20newsgroups(subset=\"train\")\n",
        "\n",
        "print(news.keys())\n",
        "\n",
        "df = pd.DataFrame(news['data'])\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    analyzer=\"word\", tokenizer=nltk.word_tokenize,\n",
        "    preprocessor=None, stop_words='english', max_features=100, max_df=.9)  \n",
        "\n",
        "tfidf_vectorizer.fit(news[\"data\"])\n",
        "\n",
        "# matrix = count_vectorizer.transform(new_sentense.split())\n",
        "# print(matrix.todense())\n",
        "print(tfidf_vectorizer.get_feature_names())\n",
        "print(tfidf_vectorizer.vocabulary_)\n",
        "\n",
        "Xtr = tfidf_vectorizer.transform(news[\"data\"])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
            "                                                   0\n",
            "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...\n",
            "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...\n",
            "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...\n",
            "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...\n",
            "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...\n",
            "['!', '#', '$', '%', '&', \"'\", \"''\", \"'ax\", \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", '*', '+', '-', '--', '...', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', ';', '<', '=', '>', '?', '[', ']', '`', '``', 'article', 'believe', 'better', 'c', 'ca', 'computer', 'did', 'distribution', 'does', 'file', 'g', 'god', 'going', 'good', 'government', 'help', 'information', 'just', 'know', 'like', 'm', 'make', 'max', 'n', \"n't\", 'need', 'new', 'nntp-posting-host', 'number', 'o', 'p', 'people', 'point', 'problem', 'q', 'question', 'r', 'really', 'reply-to', 'right', 'said', 'say', 'state', 'sure', 'thanks', 'things', 'think', 'time', 'university', 'usa', 'use', 'used', 'using', 'want', 'way', 'work', 'world', 'writes', 'x', 'year', 'years', '|']\n",
            "{\"'s\": 12, '!': 0, '?': 33, 'nntp-posting-host': 65, 'university': 86, 'really': 75, 'know': 56, 'years': 98, 'thanks': 82, '-': 16, '--': 17, 'number': 66, 'm': 58, \"n't\": 62, '<': 30, '>': 32, '...': 18, 'computer': 43, 'distribution': 45, 'usa': 87, 'way': 92, \"'m\": 10, 'new': 64, '*': 14, 'does': 46, \"'d\": 8, 'make': 59, '``': 37, \"''\": 6, 'like': 57, 'just': 55, 'better': 40, 'good': 51, 'people': 69, 'use': 88, 'question': 73, \"'ve\": 13, ';': 29, \"'ll\": 9, 'time': 85, 'world': 94, '[': 34, ']': 35, 'writes': 95, 'article': 38, 'information': 54, '&': 4, \"'\": 5, 'things': 83, 'right': 77, '$': 2, 'need': 63, 'government': 52, \"'re\": 11, 'say': 79, 'believe': 39, 'using': 90, 'year': 97, 'point': 70, 'reply-to': 76, 'sure': 81, 'file': 47, 'state': 80, '%': 3, '7': 27, '#': 1, 'problem': 71, 'said': 78, 'think': 84, 'going': 50, '1': 21, 'help': 53, 'ca': 42, 'work': 93, 'want': 91, 'god': 49, '/': 19, '4': 24, 'used': 89, 'c': 41, '3': 23, '8': 28, '2': 22, '5': 25, '6': 26, 'g': 48, 'did': 44, '|': 99, 'x': 96, 'r': 74, '0': 20, 'max': 60, '=': 31, 'o': 67, '+': 15, 'p': 68, 'n': 61, \"'ax\": 7, '`': 36, 'q': 72}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOXn5ieptKtG",
        "colab_type": "text"
      },
      "source": [
        "Applying TF-IDF on a much larger dataset of 20_news_groups dataset\n",
        "\n",
        "*max_features* means that only the top 100 words will be taken in the final repreasentation \n",
        "\n",
        "*max_df* is the threshhold for max document frequency "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7RkMpmyQnCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def top_tfidf_feats(row, features, top_n=25):\n",
        "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
        "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
        "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
        "    df = pd.DataFrame(top_feats)\n",
        "    df.columns = ['feature', 'tfidf']\n",
        "    return df\n",
        "\n",
        "def top_feats_in_doc(Xtr, features, row_id, top_n=25):\n",
        "    ''' Top tfidf features in specific document (matrix row) '''\n",
        "    row = np.squeeze(Xtr[row_id].toarray())\n",
        "    return top_tfidf_feats(row, features, top_n)\n",
        "\n",
        "\n",
        "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
        "    ''' Return the top n features that on average are most important amongst documents in rows\n",
        "        indentified by indices in grp_ids. '''\n",
        "    if grp_ids:\n",
        "        D = Xtr[grp_ids].toarray()\n",
        "    else:\n",
        "        D = Xtr.toarray()\n",
        "\n",
        "    D[D < min_tfidf] = 0\n",
        "    tfidf_means = np.mean(D, axis=0)\n",
        "    return top_tfidf_feats(tfidf_means, features, top_n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6Y7mO_Yt_Da",
        "colab_type": "text"
      },
      "source": [
        "These are some helper functions used to better analyze tf-idf representations "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DGcv_QZEvWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to clean the data \n",
        "\n",
        "def normalize(comment, lowercase=True, remove_stopwords=True):\n",
        "    if lowercase:\n",
        "        comment = comment.lower()\n",
        "    lines = comment.splitlines()\n",
        "    lines = [x.strip(' ') for x in lines]\n",
        "    lines = [x.replace('\"', '') for x in lines]\n",
        "    lines = [x.replace('\\\\\"', '') for x in lines]\n",
        "    lines = [x.replace(u'\\xa0', u'') for x in lines]\n",
        "    comment = \" \".join(lines)\n",
        "    doc = nlp(comment)\n",
        "\n",
        "    # for token in doc:\n",
        "    #   print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "    #     token.shape_, token.is_alpha, token.is_stop)\n",
        "\n",
        "    words = [token for token in doc if token.is_stop !=\n",
        "             True and token.is_punct != True]\n",
        "    # return \" \".join(words)\n",
        "    lemmatized = list()\n",
        "    for word in words:\n",
        "        lemma = word.lemma_.strip()\n",
        "        if lemma:\n",
        "            lemmatized.append(lemma)\n",
        "    return \" \".join(lemmatized)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHrszrG9uFlL",
        "colab_type": "text"
      },
      "source": [
        "cleaning data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U7JlqF5X9VL",
        "colab_type": "code",
        "outputId": "61696659-7437-4196-dc50-3c93cccda339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "data_to_process = news[\"data\"][:100]\n",
        "\n",
        "clean_data  = []\n",
        "\n",
        "\n",
        "print(\"cleaning data\")\n",
        "for row in data_to_process:\n",
        "  clean_data.append(normalize(row))\n",
        "\n",
        "print(\"data cleaned\")\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()  \n",
        "\n",
        "tfidf_vectorizer.fit(clean_data)\n",
        "\n",
        "Xtr = tfidf_vectorizer.transform(clean_data)\n",
        "\n",
        "features = tfidf_vectorizer.get_feature_names()\n",
        "\n",
        "df2 = top_feats_in_doc(Xtr, features, 0, 10)\n",
        "\n",
        "print(\"data without cleaning\")\n",
        "print(data_to_process[0])\n",
        "\n",
        "print(\"cleaned data\")\n",
        "print(clean_data[0])\n",
        "\n",
        "print(\"top features of the first document\")\n",
        "print(df2)\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "print(\"top means features across all documents\")\n",
        "df3 = top_mean_feats(Xtr, features)\n",
        "\n",
        "print(df3)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cleaning data\n",
            "data cleaned\n",
            "data without cleaning\n",
            "From: lerxst@wam.umd.edu (where's my thing)\n",
            "Subject: WHAT car is this!?\n",
            "Nntp-Posting-Host: rac3.wam.umd.edu\n",
            "Organization: University of Maryland, College Park\n",
            "Lines: 15\n",
            "\n",
            " I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "\n",
            "Thanks,\n",
            "- IL\n",
            "   ---- brought to you by your neighborhood Lerxst ----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cleaned data\n",
            "lerxst@wam.umd.edu thing subject car nntp post host rac3.wam.umd.edu organization university maryland college park line 15 wonder enlighten car see day 2-door sport car look late 60s/ early 70 call bricklin door small addition bumper separate rest body know tellme model engine spec year production car history info funky look car e mail thank il bring neighborhood lerxst\n",
            "top features of the first document\n",
            "        feature     tfidf\n",
            "0           car  0.431744\n",
            "1          door  0.278677\n",
            "2        lerxst  0.278677\n",
            "3           umd  0.255720\n",
            "4           wam  0.255720\n",
            "5          look  0.148307\n",
            "6      maryland  0.139339\n",
            "7  neighborhood  0.139339\n",
            "8      bricklin  0.139339\n",
            "9     enlighten  0.139339\n",
            "\n",
            "top means features across all documents\n",
            "      feature     tfidf\n",
            "0         com  0.019622\n",
            "1      window  0.015439\n",
            "2        scsi  0.014527\n",
            "3         car  0.012656\n",
            "4    armenian  0.011686\n",
            "5         edu  0.009681\n",
            "6        game  0.008653\n",
            "7     problem  0.008254\n",
            "8       board  0.008053\n",
            "9   insurance  0.007963\n",
            "10        war  0.007846\n",
            "11        mit  0.007841\n",
            "12    program  0.007833\n",
            "13         ca  0.007778\n",
            "14    captain  0.007454\n",
            "15        win  0.007382\n",
            "16       bike  0.007211\n",
            "17         cs  0.007127\n",
            "18    arizona  0.007084\n",
            "19    reserve  0.007057\n",
            "20       tiff  0.006922\n",
            "21       icon  0.006841\n",
            "22       font  0.006708\n",
            "23     access  0.006680\n",
            "24    stratus  0.006546\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}