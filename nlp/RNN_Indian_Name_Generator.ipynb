{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN Indian Name Generator",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manishiitg/ML_Experiments/blob/master/nlp/RNN_Indian_Name_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oUs_i8ku0G7",
        "colab_type": "text"
      },
      "source": [
        "**Generate Indian Names Randomly via Character RNNs**\n",
        "\n",
        "\n",
        "Purpose of this NN is to use character RNNs to be able to generate randome names.\n",
        "\n",
        "The NN will train on indian names data from kaggle.\n",
        "\n",
        "Network is designed to predict the next character based a sequence of previous characters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9NVDW3TGyBU",
        "colab_type": "code",
        "outputId": "3d11f17d-96ed-415e-d54a-86092c54e25b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Let's make sure the kaggle.json file is present.\n",
        "!ls -lha kaggle.json\n",
        "# Next, install the Kaggle API client.\n",
        "!pip install -q kaggle\n",
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so move it there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 66 Nov 26 12:20 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFs2lA8uUze_",
        "colab_type": "code",
        "outputId": "182ecf24-6daf-4fb2-ec19-cdf8bdfe57c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!kaggle datasets download -d chaitanyapatil7/indian-names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "indian-names.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYP4VH_pVuYY",
        "colab_type": "code",
        "outputId": "5869507c-0725-44f8-9f84-290c07d7e4da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "!unzip indian-names.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  indian-names.zip\n",
            "replace Indian-Female-Names.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: Indian-Female-Names.csv  \n",
            "replace Indian-Male-Names.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: Indian-Male-Names.csv   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoxORepk5T84",
        "colab_type": "text"
      },
      "source": [
        "Till now we have downloaded and setup our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiRgKDk3qpng",
        "colab_type": "code",
        "outputId": "afd18b1e-bf0b-4ec4-b32c-944b1b67f9d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23mLDiUNWmoh",
        "colab_type": "code",
        "outputId": "c572ab64-c890-4684-c74c-98aec7ad41d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'-\"\n",
        "\n",
        "def findFiles(path): return glob.glob(path)\n",
        "# Read a file and split into lines\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "def readLines(filename):\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    lines = [line.split(\",\")[0].strip() for line in lines]\n",
        "    return [unicodeToAscii(line) for line in lines]\n",
        "\n",
        "# Build the category_lines dictionary, a list of lines per category\n",
        "\n",
        "lines1 = readLines(\"Indian-Female-Names.csv\")\n",
        "lines2 = readLines(\"Indian-Male-Names.csv\")\n",
        "\n",
        "print(\"female names\", len(lines1))\n",
        "print(\"male names\", len(lines2))\n",
        "\n",
        "lines = lines1 + lines2\n",
        "\n",
        "print(\"total names\", len(lines))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "female names 15383\n",
            "male names 14846\n",
            "total names 30229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KvgRKpn5iSf",
        "colab_type": "text"
      },
      "source": [
        "Read the names from the csv files "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RetauFpiYjKx",
        "colab_type": "code",
        "outputId": "48473923-6419-4c46-d774-243561a69d85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "data = []\n",
        "for l in lines:\n",
        "   data += list(l)\n",
        "\n",
        "chars = list(set(data))\n",
        "\n",
        "# start_char = \"<\"\n",
        "# end_char = \">\"\n",
        "\n",
        "# chars.append(start_char)\n",
        "# chars.append(end_char)\n",
        "\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "\n",
        "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "\n",
        "n_chars = len(chars)\n",
        "\n",
        "print(chars)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data has 274545 characters, 29 unique.\n",
            "['e', 'p', 'd', 's', 'f', 'q', 't', ' ', 'v', 'y', '.', 'u', 'r', 'i', 'h', 'k', 'c', 'j', 'b', 'a', 'n', 'z', 'o', 'w', 'm', 'x', 'l', '-', 'g']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sl2WMeWZlK7",
        "colab_type": "code",
        "outputId": "c98b3dc9-3acd-4f2b-b586-0e8a3d33d306",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "char_to_ix = {ch:i for i, ch in enumerate(chars)}\n",
        "ix_to_char = {i:ch for i, ch in enumerate(chars)}\n",
        "print('char_to_ix', char_to_ix)\n",
        "print('ix_to_char', ix_to_char)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "char_to_ix {'e': 0, 'p': 1, 'd': 2, 's': 3, 'f': 4, 'q': 5, 't': 6, ' ': 7, 'v': 8, 'y': 9, '.': 10, 'u': 11, 'r': 12, 'i': 13, 'h': 14, 'k': 15, 'c': 16, 'j': 17, 'b': 18, 'a': 19, 'n': 20, 'z': 21, 'o': 22, 'w': 23, 'm': 24, 'x': 25, 'l': 26, '-': 27, 'g': 28}\n",
            "ix_to_char {0: 'e', 1: 'p', 2: 'd', 3: 's', 4: 'f', 5: 'q', 6: 't', 7: ' ', 8: 'v', 9: 'y', 10: '.', 11: 'u', 12: 'r', 13: 'i', 14: 'h', 15: 'k', 16: 'c', 17: 'j', 18: 'b', 19: 'a', 20: 'n', 21: 'z', 22: 'o', 23: 'w', 24: 'm', 25: 'x', 26: 'l', 27: '-', 28: 'g'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-Cbg-2MtU4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers, max_seq_length):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Defining some parameters\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        #Defining the layers\n",
        "        # RNN Layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
        "\n",
        "        # https://discuss.pytorch.org/t/could-someone-explain-batch-first-true-in-lstm/15402\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim * max_seq_length, output_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        #Initializing hidden state for first input using method defined below\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        # Passing in the input and hidden state into the model and obtaining outputs\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        # print(out.shape, 'rnn output snape')\n",
        "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
        "        # basically we are concating all the letters together. \n",
        "        # the size would be hidden_dim * max_seq_length\n",
        "        # since our purpose to predect the next character i.e a single character \n",
        "        # our network should output only a single character\n",
        "        out = out.contiguous().view(batch_size, -1)\n",
        "        # print(out.shape, \" new out shape\")\n",
        "        \n",
        "        \n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # print(out.shape, 'single layer output shape')\n",
        "        \n",
        "        \n",
        "        out = self.softmax(out)\n",
        "\n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
        "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02086pDptYiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_length = 20\n",
        "# this will maximum length of a name\n",
        "# this is required to define the network\n",
        "\n",
        "model = Model(input_size=n_chars, output_size=n_chars, hidden_dim=12, n_layers=1, max_seq_length=max_seq_length)\n",
        "\n",
        "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 1000\n",
        "lr=0.001\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9hiEzA3coj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode_batch(sequence, dict_size, seq_len, batch_size):\n",
        "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
        "\n",
        "    # we need to have all seq of same length or else cannot create this array\n",
        "    # if we don't have batch_size i.e if we process one input at a time. we \n",
        "    # don't need to have sequence of same length\n",
        "\n",
        "    tensor = torch.zeros(batch_size, seq_len, dict_size)\n",
        "\n",
        "    # len(sequence) and seq_len will be same in case when we are standaring the lenght!\n",
        "\n",
        "    # features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
        "    \n",
        "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
        "    for i in range(batch_size):\n",
        "        for li in range(len(sequence[i])):\n",
        "          tensor[i][li][sequence[i][li]] = 1\n",
        "    return tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0XY_7rKbAZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# general utility function but not used as such\n",
        "def one_hot_encode(line, dict_size):\n",
        "    tensor = torch.zeros(len(line), dict_size)\n",
        "    for li in range(len(line)):\n",
        "        tensor[li][line[li]] = 1\n",
        "    return tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us7FsaDDrYyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def targetTensor(seq):\n",
        "  tensor = torch.zeros(len(seq))\n",
        "  for i in range(len(seq)):\n",
        "    line = seq[i]\n",
        "    tensor[i] = line\n",
        "  return tensor.long()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzRBVmXW7hd0",
        "colab_type": "text"
      },
      "source": [
        "Target tensor is a simple tensor of indexes.\n",
        "The reason for this because we are use NN Loss, and for we need to provide indexes of character. Its basically is a classification problem with characters a label. \n",
        "\n",
        "So our network will output software of possible charaters and we need to compare that with index's of the actual character \n",
        "\n",
        "This is important to understand"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un74TM8vj04w",
        "colab_type": "code",
        "outputId": "90cfea78-b3fc-44fc-9142-8a5f668ffcfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "source": [
        "def train(epoch = -1):\n",
        "  tloss = 0\n",
        "  itrloss = 0\n",
        "  for line_no in range(len(lines)):\n",
        "    name = lines[line_no]\n",
        "    name = name.strip()\n",
        "\n",
        "    if len(name) <= 1:\n",
        "      continue\n",
        "\n",
        "    input_seq_idx = []\n",
        "    target_char_idx = []\n",
        "    if len(name) >= max_seq_length: \n",
        "      # print(name , \" bigger than max seq length FYI\")\n",
        "      name = name[0:max_seq_length - 1]\n",
        "    \n",
        "    for i in range(1, len(name)):\n",
        "      # input_seq = name[0:i] if i != 0 else start_char\n",
        "      input_seq = name[0:i]\n",
        "      target_char = name[i]\n",
        "\n",
        "      # input_seq = input_seq + end_char\n",
        "      input_seq = input_seq\n",
        "\n",
        "      # print(input_seq,\" ==== \", target_char)\n",
        "\n",
        "      input_seq_idx.append([char_to_ix[ch] for ch in input_seq])\n",
        "      target_char_idx.append(char_to_ix[target_char])\n",
        "\n",
        "    input_encoded = one_hot_encode_batch(input_seq_idx, n_chars, max_seq_length, len(input_seq_idx))\n",
        "    target_encoded = targetTensor(target_char_idx)\n",
        "\n",
        "    input_encoded = input_encoded.to(device)\n",
        "    target_encoded = target_encoded.to(device)\n",
        "    # print(input_encoded.shape, \"input shape\")\n",
        "    \n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output, hidden = model(input_encoded)\n",
        "    output = output.to(device)\n",
        "    # output = output\n",
        "\n",
        "    # print(output.shape, 'final nn ouput shape')\n",
        "    # print(target_encoded.shape, \"target shape\")\n",
        "    # print(target_encoded, \"target\")\n",
        "\n",
        "    # The input is expected to contain scores for each class.\n",
        "    # input has to be a 2D Tensor of size (minibatch, C).\n",
        "    # This criterion expects a class index (0 to C-1) as the target for each value of a 1D tensor of size minibatch\n",
        "\n",
        "    loss = criterion(output, target_encoded)\n",
        "    loss.backward() # Does backpropagation and calculates gradients\n",
        "    optimizer.step() # Updates the weights accordingly\n",
        "    tloss += loss \n",
        "    itrloss += loss\n",
        "\n",
        "    \n",
        "\n",
        "    if line_no%1000 == 0 and epoch == -1:\n",
        "      print(\"Iteration Loss: {}/{}...\".format(line_no, itrloss/1000))\n",
        "      prediction = output.argmax(dim=1)\n",
        "      itrloss = 0\n",
        "\n",
        "  print(\"Epoch: {}/ Loss: {}\".format(epoch, tloss / len(lines)))\n",
        "  return tloss\n",
        "\n",
        "train()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration Loss: 0/0.002708323299884796...\n",
            "Iteration Loss: 1000/2.239064931869507...\n",
            "Iteration Loss: 2000/2.2679617404937744...\n",
            "Iteration Loss: 3000/2.25607967376709...\n",
            "Iteration Loss: 4000/2.2140843868255615...\n",
            "Iteration Loss: 5000/2.248345375061035...\n",
            "Iteration Loss: 6000/2.2255237102508545...\n",
            "Iteration Loss: 7000/2.2739601135253906...\n",
            "Iteration Loss: 8000/2.292475700378418...\n",
            "Iteration Loss: 9000/2.263462781906128...\n",
            "Iteration Loss: 10000/2.2794179916381836...\n",
            "Iteration Loss: 11000/2.2275888919830322...\n",
            "Iteration Loss: 12000/2.2459630966186523...\n",
            "Iteration Loss: 13000/2.2625675201416016...\n",
            "Iteration Loss: 14000/2.215899705886841...\n",
            "Iteration Loss: 15000/2.234445095062256...\n",
            "Iteration Loss: 16000/2.498283863067627...\n",
            "Iteration Loss: 17000/2.4593005180358887...\n",
            "Iteration Loss: 18000/2.4239258766174316...\n",
            "Iteration Loss: 19000/2.378148317337036...\n",
            "Iteration Loss: 20000/2.383873224258423...\n",
            "Iteration Loss: 21000/2.3793511390686035...\n",
            "Iteration Loss: 22000/2.4287781715393066...\n",
            "Iteration Loss: 23000/2.382899761199951...\n",
            "Iteration Loss: 24000/2.3884730339050293...\n",
            "Iteration Loss: 25000/2.3594813346862793...\n",
            "Iteration Loss: 26000/2.3458688259124756...\n",
            "Iteration Loss: 27000/2.3626961708068848...\n",
            "Iteration Loss: 28000/2.335481882095337...\n",
            "Iteration Loss: 29000/2.3806986808776855...\n",
            "Iteration Loss: 30000/2.3460028171539307...\n",
            "Epoch: -1/ Loss: 2.3196260929107666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(70119.9766, device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcGzmpcuPkSv",
        "colab_type": "code",
        "outputId": "350288d8-bad1-48ae-b4c5-970716cef441",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import random\n",
        "\n",
        "def eval():\n",
        "  x = random.randrange(100,500)\n",
        "  # print(\"index\" , x)\n",
        "  eval_lines = lines[x:x+200]\n",
        "\n",
        "  input_seq_idx = []\n",
        "  target_char_idx = []\n",
        "  for line_no in range(len(eval_lines)):\n",
        "      name = lines[line_no]\n",
        "      name = name.strip()\n",
        "\n",
        "      if len(name) == 0:\n",
        "        continue\n",
        "\n",
        "      if len(name) >= max_seq_length: \n",
        "        name = name[0:max_seq_length - 1]\n",
        "      \n",
        "      for i in range(1, len(name)):\n",
        "        input_seq = name[0:i]\n",
        "        target_char = name[i]\n",
        "\n",
        "        # input_seq = input_seq + end_char\n",
        "        input_seq = input_seq\n",
        "\n",
        "        input_seq_idx.append([char_to_ix[ch] for ch in input_seq])\n",
        "        target_char_idx.append(char_to_ix[target_char])\n",
        "\n",
        "  input_encoded = one_hot_encode_batch(input_seq_idx, n_chars, max_seq_length, len(input_seq_idx))\n",
        "  target_encoded = targetTensor(target_char_idx)\n",
        "\n",
        "  input_encoded = input_encoded.to(device)\n",
        "  target_encoded = target_encoded.to(device)\n",
        "\n",
        "  # print(input_encoded.shape, \"input shape\")\n",
        "  # print(target_encoded.shape, \"target shape\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    output, _ = model(input_encoded)\n",
        "\n",
        "    output = output.to(device)\n",
        "\n",
        "    prediction = output.argmax(dim=1).cpu()\n",
        "    acc = accuracy_score(target_encoded.cpu(), prediction)\n",
        "    print(\"Accuracy {}\".format(acc))\n",
        "\n",
        "\n",
        "eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 0.23433048433048434\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a73RyzVgPgMY",
        "colab_type": "code",
        "outputId": "7f35cc48-76af-4086-af83-e22e586c25eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        }
      },
      "source": [
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "  train(epoch)\n",
        "  eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0/ Loss: 2.3081984519958496\n",
            "Accuracy 0.22435897435897437\n",
            "Epoch: 1/ Loss: 2.309570789337158\n",
            "Accuracy 0.22293447293447294\n",
            "Epoch: 2/ Loss: 2.308520555496216\n",
            "Accuracy 0.22863247863247863\n",
            "Epoch: 3/ Loss: 2.3068556785583496\n",
            "Accuracy 0.23717948717948717\n",
            "Epoch: 4/ Loss: 2.306570053100586\n",
            "Accuracy 0.2571225071225071\n",
            "Epoch: 5/ Loss: 2.3057808876037598\n",
            "Accuracy 0.2336182336182336\n",
            "Epoch: 6/ Loss: 2.3072760105133057\n",
            "Accuracy 0.23575498575498577\n",
            "Epoch: 7/ Loss: 2.307806968688965\n",
            "Accuracy 0.23005698005698005\n",
            "Epoch: 8/ Loss: 2.306295394897461\n",
            "Accuracy 0.2336182336182336\n",
            "Epoch: 9/ Loss: 2.308224678039551\n",
            "Accuracy 0.22863247863247863\n",
            "Epoch: 10/ Loss: 2.30735182762146\n",
            "Accuracy 0.21652421652421652\n",
            "Epoch: 11/ Loss: 2.308230400085449\n",
            "Accuracy 0.22934472934472935\n",
            "Epoch: 12/ Loss: 2.3063833713531494\n",
            "Accuracy 0.23717948717948717\n",
            "Epoch: 13/ Loss: 2.3082544803619385\n",
            "Accuracy 0.21866096866096865\n",
            "Epoch: 14/ Loss: 2.308337926864624\n",
            "Accuracy 0.21723646723646722\n",
            "Epoch: 15/ Loss: 2.3048954010009766\n",
            "Accuracy 0.2443019943019943\n",
            "Epoch: 16/ Loss: 2.306457757949829\n",
            "Accuracy 0.21937321937321938\n",
            "Epoch: 17/ Loss: 2.306788444519043\n",
            "Accuracy 0.24074074074074073\n",
            "Epoch: 18/ Loss: 2.30692720413208\n",
            "Accuracy 0.22792022792022792\n",
            "Epoch: 19/ Loss: 2.3061187267303467\n",
            "Accuracy 0.24572649572649571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kTZ-4IVjakH",
        "colab_type": "code",
        "outputId": "5723c407-1699-4d76-e825-980be7c09c31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        }
      },
      "source": [
        "def generate(input_seq_sample):\n",
        "  # input_seq_sample = input_seq_sample + end_char\n",
        "  input_seq_sample = input_seq_sample\n",
        "  # print(input_seq_sample)\n",
        "  input_seq_idx = []\n",
        "  input_seq_idx.append([char_to_ix[ch] for ch in input_seq_sample])\n",
        "  input_encoded = one_hot_encode_batch(input_seq_idx, n_chars, max_seq_length, len(input_seq_idx))\n",
        "\n",
        "  input_encoded = input_encoded.to(device)\n",
        "\n",
        "  output, _ = model(input_encoded)\n",
        "\n",
        "  prediction = output.argmax(dim=1).cpu().numpy().data\n",
        "\n",
        "  text = [ix_to_char[idx] for idx in prediction ]\n",
        "  # print(text)\n",
        "\n",
        "  return \"\".join(text)\n",
        "\n",
        "\n",
        "def generate_name(input_seq_sample, name_length):\n",
        "  print(\"generate name with starting seq '{}' and of size '{}'\".format(input_seq_sample, name_length))\n",
        "\n",
        "  for i in range(name_length):\n",
        "    ret = generate(input_seq_sample)\n",
        "    # print(input_seq_sample, \" :::: \", ret)\n",
        "    input_seq_sample += ret\n",
        "\n",
        "  print(\"Name Generated {}\".format(input_seq_sample))\n",
        "\n",
        "  return input_seq_sample\n",
        "\n",
        "def run_random_generation():\n",
        "  init_seq_size = random.randrange(0,5)\n",
        "  init_seq_index = random.randrange(0, len(chars) - init_seq_size)\n",
        "  init_seq = chars[init_seq_index: init_seq_index + init_seq_size]\n",
        "\n",
        "  input_seq_sample = \"\".join(init_seq)\n",
        "  name_length = random.randrange(1, max_seq_length-len(input_seq_sample))\n",
        "\n",
        "  generate_name(input_seq_sample,name_length)\n",
        "\n",
        "  \n",
        "\n",
        "for i in range(10):\n",
        "  run_random_generation()\n",
        "\n",
        "generate_name(\"manish\", 10)\n",
        "generate_name(\"arun\", 10)\n",
        "generate_name(\"mahima\", 10)\n",
        "\n",
        "generate_name(\"dee\", 3)\n",
        "generate_name(\"man\", 3)\n",
        "generate_name(\"mahi\", 2)\n",
        "generate_name(\"z\", 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generate name with starting seq '' and of size '11'\n",
            "Name Generated ashak kumar\n",
            "generate name with starting seq '' and of size '11'\n",
            "Name Generated ashash kuma\n",
            "generate name with starting seq 'ihk' and of size '3'\n",
            "Name Generated ihkar \n",
            "generate name with starting seq 'hk' and of size '16'\n",
            "Name Generated hkarekumarhararama\n",
            "generate name with starting seq 'owm' and of size '2'\n",
            "Name Generated owmat\n",
            "generate name with starting seq 'sfq' and of size '11'\n",
            "Name Generated sfqoododoreded\n",
            "generate name with starting seq ' ' and of size '4'\n",
            "Name Generated  rush\n",
            "generate name with starting seq 'l' and of size '6'\n",
            "Name Generated laxandi\n",
            "generate name with starting seq 'sfqt' and of size '12'\n",
            "Name Generated sfqtayanondadada\n",
            "generate name with starting seq ' vy' and of size '2'\n",
            "Name Generated  vyip\n",
            "generate name with starting seq 'manish' and of size '10'\n",
            "Name Generated manish kumararat\n",
            "generate name with starting seq 'arun' and of size '10'\n",
            "Name Generated aruns hhakumar\n",
            "generate name with starting seq 'mahima' and of size '10'\n",
            "Name Generated mahimash dudhary\n",
            "generate name with starting seq 'dee' and of size '3'\n",
            "Name Generated deepak\n",
            "generate name with starting seq 'man' and of size '3'\n",
            "Name Generated manud \n",
            "generate name with starting seq 'mahi' and of size '2'\n",
            "Name Generated mahis \n",
            "generate name with starting seq 'z' and of size '5'\n",
            "Name Generated zakesh\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'zakesh'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMtQivVM9_Su",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# criterion = nn.NLLLoss()\n",
        "\n",
        "# output = torch.randn(10, 120).float()\n",
        "# target = torch.FloatTensor(10).uniform_(0, 120).long()\n",
        "\n",
        "# print(output.shape)\n",
        "# print(target.shape)\n",
        "# print(target)\n",
        "\n",
        "# loss = criterion(output, target)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}