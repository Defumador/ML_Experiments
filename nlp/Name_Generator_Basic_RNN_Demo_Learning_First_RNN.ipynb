{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Name Generator - Basic RNN Demo Learning First RNN",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manishiitg/ML_Experiments/blob/master/nlp/Name_Generator_Basic_RNN_Demo_Learning_First_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXkIrXOuHt2A",
        "colab_type": "text"
      },
      "source": [
        "This is my first implementation on RNN and tryout to see how different it works\n",
        "\n",
        "There are some the blogs which i followed to understand\n",
        "\n",
        "https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/\n",
        "\n",
        "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "\n",
        "https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
        "\n",
        "\n",
        "This doc is to get a very inner level working on RNN and to understand them\n",
        "\n",
        "\n",
        "We are developing a Character level RNN, what this means is our network will try to predict the next character based on the previous characters \n",
        "\n",
        "The output for this network is not very good, as there is not much training data and its a very simple network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzzqrpWjHp-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFI6-vXEIFq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = ['hey how are you','good i am fine','have a nice day']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5arHuDzQLVr9",
        "colab_type": "code",
        "outputId": "e823d212-d42f-4b4d-e7ef-f4e1405db2c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import unicodedata\n",
        "import string\n",
        "\n",
        "# all_letters = string.ascii_letters + \" .,;'\"\n",
        "\n",
        "all_letters = \"\".join(list(set(''.join(text))))\n",
        "\n",
        "\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "print(all_letters)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "yhiwuarmfnovcegd \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M-JSfZYCI2c",
        "colab_type": "text"
      },
      "source": [
        "We define all our letters here. In current case its just all alphabets and some special characters. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfWRsSRLCc4L",
        "colab_type": "text"
      },
      "source": [
        "This is sample text we wil try to model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dj56iq7JOZz",
        "colab_type": "code",
        "outputId": "6e8f92cc-a0e4-47eb-b8a1-40977ce99fd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "maxlen = len(max(text, key=len))\n",
        "print(\"The longest string has {} characters\".format(maxlen))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The longest string has 15 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPcJ5f14MgWz",
        "colab_type": "code",
        "outputId": "307353aa-a8c4-44b4-d681-bb476594c753",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "def one_hot_encode(line):\n",
        "    tensor = torch.zeros(len(line), n_letters)\n",
        "    for li in range(len(line)):\n",
        "        letter = line[li]\n",
        "        tensor[li][all_letters.find(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "one_hot_encode(\"hello\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM7c-i44Dnp7",
        "colab_type": "text"
      },
      "source": [
        "Converting all sequences to same length, this is required only if we are doing batch processing of data because we need to have tensors of fixed length. if we always process one sequence at a time this is not needed\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_KyEn93Ul_u",
        "colab_type": "code",
        "outputId": "13a6256b-9e43-49df-f3f8-2eb3499fae10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "input_seq = []\n",
        "target_seq = []\n",
        "\n",
        "for i in range(len(text)):\n",
        "    # Remove last character for input sequence\n",
        "    input_seq.append(text[i][:-1])\n",
        "    \n",
        "    # Remove firsts character for target sequence\n",
        "    target_seq.append(text[i][1:])\n",
        "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))\n",
        "\n",
        "print(input_seq)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Sequence: hey how are yo\n",
            "Target Sequence: ey how are you\n",
            "Input Sequence: good i am fin\n",
            "Target Sequence: ood i am fine\n",
            "Input Sequence: have a nice da\n",
            "Target Sequence: ave a nice day\n",
            "['hey how are yo', 'good i am fin', 'have a nice da']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-X_hWh-MSE2",
        "colab_type": "code",
        "outputId": "325a09ff-4246-4985-ab29-91d6f25e1f16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def normalize_length(text):\n",
        "  for i in range(len(text)):\n",
        "      while len(text[i])<maxlen:\n",
        "          text[i] += ' '\n",
        "  return text\n",
        "\n",
        "input_seq = normalize_length(input_seq)\n",
        "target_seq = normalize_length(target_seq)\n",
        "print(input_seq)\n",
        "print(target_seq)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hey how are yo ', 'good i am fin  ', 'have a nice da ']\n",
            "['ey how are you ', 'ood i am fine  ', 'ave a nice day ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMF40LwPDzOT",
        "colab_type": "text"
      },
      "source": [
        "Our RNN purpose is to predict the next character given a set of previous characters. In current case we take a sentence and remove the last character from the input seq and remove the first character from target seq. \n",
        "\n",
        "**RNN will automatically process then one character at a time!!??**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfD5pUDLg9dx",
        "colab_type": "code",
        "outputId": "a745961d-0a16-4a30-8bdb-a4130a570cdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "dict_size = n_letters\n",
        "seq_len = maxlen\n",
        "batch_size = len(text)\n",
        "\n",
        "def one_hot_encode_batch(sequence, dict_size, seq_len, batch_size):\n",
        "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
        "\n",
        "    # we need to have all seq of same length or else cannot create this array\n",
        "    # if we don't have batch_size i.e if we process one input at a time. we \n",
        "    # don't need to have sequence of same length\n",
        "\n",
        "    tensor = torch.zeros(batch_size, seq_len, dict_size)\n",
        "\n",
        "    # len(sequence) and seq_len will be same in case when we are standaring the lenght!\n",
        "\n",
        "    # features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
        "    \n",
        "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
        "    for i in range(batch_size):\n",
        "        for li in range(len(sequence[i])):\n",
        "          letter = sequence[i][li]\n",
        "          tensor[i][li][all_letters.find(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "print(input_seq, \"input seq\")\n",
        "input_seq_tensor = one_hot_encode_batch(input_seq, dict_size, seq_len, batch_size)\n",
        "print(input_seq_tensor.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hey how are yo ', 'good i am fin  ', 'have a nice da '] input seq\n",
            "torch.Size([3, 15, 17])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKSkKeO9PgUD",
        "colab_type": "text"
      },
      "source": [
        "One hot encoding our entire batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZnF4t8csumg",
        "colab_type": "code",
        "outputId": "af0744c9-de62-43cf-a1af-528a74ca1cbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "def targetTensor(seq, seq_len):\n",
        "  tensor = torch.zeros(len(seq), seq_len)\n",
        "  for i in range(len(seq)):\n",
        "    line = seq[i]\n",
        "    for li in range(len(line)):\n",
        "      tensor[i][li] = all_letters.find(line[li])\n",
        "    # letter_indexes.append(n_letters - 1) # EOS\n",
        "    # tensor[i][-1] = n_letters - 1\n",
        "  return tensor\n",
        "\n",
        "\n",
        "print(targetTensor([\"hello\"], seq_len))\n",
        "print(target_seq)\n",
        "target_seq_tensor = targetTensor(target_seq, seq_len)\n",
        "print(target_seq_tensor)\n",
        "print(target_seq_tensor.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1., 13., -1., -1., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.]])\n",
            "['ey how are you ', 'ood i am fine  ', 'ave a nice day ']\n",
            "tensor([[13.,  0., 16.,  1., 10.,  3., 16.,  5.,  6., 13., 16.,  0., 10.,  4.,\n",
            "         16.],\n",
            "        [10., 10., 15., 16.,  2., 16.,  5.,  7., 16.,  8.,  2.,  9., 13., 16.,\n",
            "         16.],\n",
            "        [ 5., 11., 13., 16.,  5., 16.,  9.,  2., 12., 13., 16., 15.,  5.,  0.,\n",
            "         16.]])\n",
            "torch.Size([3, 15])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1iILRfRPiXg",
        "colab_type": "text"
      },
      "source": [
        "Target tensor is not one hot encoded "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZivsC3cLou9c",
        "colab_type": "code",
        "outputId": "1ecf1d65-a5bc-4de5-fa44-dec0dc22f9af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def convert_one_hot_to_text(one_hot):\n",
        "  text = ''\n",
        "  for i in range(len(one_hot)):\n",
        "    for li in range(len(one_hot[i])):\n",
        "      if one_hot[i][li] == 1:\n",
        "        text += all_letters[li]\n",
        "  \n",
        "  return text\n",
        "\n",
        "def convert_index_to_text(target_seq):\n",
        "  text = ''\n",
        "  for i in range(len(target_seq)):\n",
        "    letter_index = int(target_seq[i].numpy().item())\n",
        "    # if letter_index > 0 and letter_index != n_letters - 1:\n",
        "    if letter_index > 0:\n",
        "      text += all_letters[letter_index]\n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "print(convert_one_hot_to_text(input_seq_tensor[0]))  \n",
        "print(convert_index_to_text(target_seq_tensor[0]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hey how are yo \n",
            "e how are ou \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wayE7dTXQSpl",
        "colab_type": "text"
      },
      "source": [
        "Utility functions to see if we can decode our text back properly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxrLaYVb1WAv",
        "colab_type": "code",
        "outputId": "c2a9f5ac-d8a6-47a1-e893-7673db84c36e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjot8TZgUb_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Defining some parameters\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        #Defining the layers\n",
        "        # RNN Layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        #Initializing hidden state for first input using method defined below\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        # Passing in the input and hidden state into the model and obtaining outputs\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "\n",
        "        # print(out.shape, 'rnn output snape')\n",
        "        \n",
        "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
        "        out = out.contiguous().view(-1, self.hidden_dim)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # print(out.shape, 'single layer output shape')\n",
        "        \n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
        "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
        "        return hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td33FF841cmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
        "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 1000\n",
        "lr=0.01\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS03GxCC1ipb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "f270dc3c-8214-4889-ac57-a5adbe0707f0"
      },
      "source": [
        "input_seq_tensor = input_seq_tensor.to(device)\n",
        "target_seq_tensor = target_seq_tensor.to(device)\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "    output, hidden = model(input_seq_tensor)\n",
        "    output = output.to(device)\n",
        "    # print(output.shape, 'NN output shape')\n",
        "    # print(target_seq_tensor.long().view(-1).shape, \"target seq shape\")\n",
        "    loss = criterion(output, target_seq_tensor.long().view(-1))\n",
        "    loss.backward() # Does backpropagation and calculates gradients\n",
        "    optimizer.step() # Updates the weights accordingly\n",
        "    \n",
        "    if epoch%100 == 0:\n",
        "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "        print(\"Loss: {:.4f}\".format(loss.item()))\n",
        "        print(output.shape, 'NN output shape')\n",
        "        prob = nn.functional.softmax(output[-1], dim=0).data\n",
        "        text = ''\n",
        "        for x in range(len(prob)):\n",
        "          if prob[x] > 0:\n",
        "            text += all_letters[x]\n",
        "\n",
        "        \n",
        "        print(\"network output\" , text)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100/1000............. Loss: -14.0793\n",
            "torch.Size([45, 17]) NN output shape\n",
            "network output yhiwuarmfnovcegd \n",
            "Epoch: 200/1000............. Loss: -27.5910\n",
            "torch.Size([45, 17]) NN output shape\n",
            "network output yhiwuarmfnovcegd \n",
            "Epoch: 300/1000............. Loss: -40.8678\n",
            "torch.Size([45, 17]) NN output shape\n",
            "network output yhiwuarmfnovcegd \n",
            "Epoch: 400/1000............. Loss: -54.0519\n",
            "torch.Size([45, 17]) NN output shape\n",
            "network output yhiwuarmfnovcegd \n",
            "Epoch: 500/1000............. Loss: -67.1864\n",
            "torch.Size([45, 17]) NN output shape\n",
            "network output yhiwuarmfnovcegd \n",
            "Epoch: 600/1000............. Loss: -80.2900\n",
            "torch.Size([45, 17]) NN output shape\n",
            "network output yhiwuarmfnovcegd \n",
            "Epoch: 700/1000............. Loss: -93.3727\n",
            "torch.Size([45, 17]) NN output shape\n",
            "network output yhiwuarmfnovcegd \n",
            "Epoch: 800/1000............. Loss: -106.4402\n",
            "torch.Size([45, 17]) NN output shape\n",
            "network output yhiwuarmfnovced \n",
            "Epoch: 900/1000............. Loss: -119.4965\n",
            "torch.Size([45, 17]) NN output shape\n",
            "network output yhiwuarmfnovced \n",
            "Epoch: 1000/1000............. Loss: -132.5440\n",
            "torch.Size([45, 17]) NN output shape\n",
            "network output yhiwuarmfnovced \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJDCx-2OYnqw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "c6d0a68f-399c-4abc-a14f-e21beab6efa4"
      },
      "source": [
        "print(input_seq_tensor.shape, 'INPUT TENSOR SHAPE')\n",
        "print(target_seq_tensor[0])\n",
        "print(target_seq_tensor.shape, 'TARGET TENSOR SHAPE')\n",
        "print(target_seq_tensor.long().view(-1).shape, \"target seq shape\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 15, 17]) INPUT TENSOR SHAPE\n",
            "tensor([13.,  0., 16.,  1., 10.,  3., 16.,  5.,  6., 13., 16.,  0., 10.,  4.,\n",
            "        16.])\n",
            "torch.Size([3, 15]) TARGET TENSOR SHAPE\n",
            "torch.Size([45]) target seq shape\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eivhbc1mYMvt",
        "colab_type": "text"
      },
      "source": [
        "One thing in this didn't make sense to me for long time. \n",
        "\n",
        " \n",
        "\n",
        "*   The target tensor is of shape 3x15  i.e batch_size 3 and 15 dim array which has indexes for every letter. We also call view(-1) to reshare the tensor to shape 45\n",
        "*   The output of the NN is of shape torch.Size([45, 57]) NN output shape in this 57 is the dict size.\n",
        "* We are calling NNLoss on this. \n",
        "* The target tensor has the 3 * 15 sentenses reshaped to 45 i.e all sentenses concated \n",
        "* The share 45x57 is using NNLoss function, so interally it will call a softmax and convert it to 45x1 \n",
        "* Hence the Loss works. \n",
        "\n",
        "https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-JW9POekxh6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5796d7c4-1d97-4cf0-e7fd-bf315a793be9"
      },
      "source": [
        "input_seq_tensor_test = one_hot_encode_batch([\"good\"], dict_size, 4, 1)\n",
        "print(input_seq_tensor_test.shape)\n",
        "print(convert_one_hot_to_text(input_seq_tensor_test[0]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 17])\n",
            "good\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQdw3jqyhRr_",
        "colab_type": "code",
        "outputId": "701bb8d1-6dc7-444a-b1ce-aa6195e2a79e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "def predict(model, character):\n",
        "    # One-hot encoding our input to fit into the model\n",
        "    # character = np.array([[char2int[c] for c in character]])\n",
        "\n",
        "    print(character)\n",
        "    # print(len(character))\n",
        "    character = one_hot_encode_batch([character], dict_size, len(character), 1)\n",
        "    # print(character[0].shape)\n",
        "    # print(character[0])\n",
        "    # print(convert_one_hot_to_text(character[0]), \"converted\")  \n",
        "    # return\n",
        "\n",
        "    character = character.to(device)\n",
        "    \n",
        "    out, hidden = model(character)\n",
        "\n",
        "    # print(out.shape)\n",
        "\n",
        "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
        "\n",
        "\n",
        "    # Taking the class with the highest probability score from the output\n",
        "    char_ind = torch.max(prob, dim=0)[1].item()\n",
        "\n",
        "    print(all_letters[char_ind])\n",
        "\n",
        "    return all_letters[char_ind], hidden\n",
        "\n",
        "def sample(model, out_len, start='hey'):\n",
        "    model.eval() # eval mode\n",
        "    start = start.lower()\n",
        "    # First off, run through the starting characters\n",
        "    chars = [ch for ch in start]\n",
        "    size = out_len - len(chars)\n",
        "    # Now pass in the previous characters and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(model, \"\".join(chars))\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)\n",
        "\n",
        "sample(model, 15, 'good')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "good\n",
            "o\n",
            "goodo\n",
            "o\n",
            "goodoo\n",
            "o\n",
            "goodooo\n",
            "o\n",
            "goodoooo\n",
            "o\n",
            "goodooooo\n",
            "o\n",
            "goodoooooo\n",
            "o\n",
            "goodooooooo\n",
            "o\n",
            "goodoooooooo\n",
            "o\n",
            "goodooooooooo\n",
            "o\n",
            "goodoooooooooo\n",
            "o\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'goodooooooooooo'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}