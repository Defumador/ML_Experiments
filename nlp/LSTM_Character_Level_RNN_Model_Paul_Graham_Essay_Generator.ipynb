{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM Character Level RNN Model Paul Graham Essay Generator",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manishiitg/ML_Experiments/blob/master/nlp/LSTM_Character_Level_RNN_Model_Paul_Graham_Essay_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oUs_i8ku0G7",
        "colab_type": "text"
      },
      "source": [
        "** Understanding Basic LSTM with Character Level Language Model  **\n",
        "\n",
        "Basic understanding - https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n",
        "Will try to reproduce results from Karpathy amazing blog post http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "\n",
        "\n",
        "\"Technical: Lets train a 2-layer LSTM with 512 hidden nodes (approx. 3.5 million parameters), and with dropout of 0.5 after each layer. We’ll train with batches of 100 examples and truncated backpropagation through time of length 100 characters. With these settings one batch on a TITAN Z GPU takes about 0.46 seconds (this can be cut in half with 50 character BPTT at negligible cost in performance). Without further ado, lets see a sample from the RNN:\"\n",
        "\n",
        "\n",
        "**Mainly notice how to use hidden state, because of hidden state misuse, this took very long **\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9NVDW3TGyBU",
        "colab_type": "code",
        "outputId": "8a15dabf-f1bd-42e0-86cc-8d30648cc9cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Let's make sure the kaggle.json file is present.\n",
        "!ls -lha kaggle.json\n",
        "# Next, install the Kaggle API client.\n",
        "!pip install -q kaggle\n",
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so move it there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 66 Nov 29 17:08 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFs2lA8uUze_",
        "colab_type": "code",
        "outputId": "657d8d95-f8a0-4bec-fdc4-b1137529ce33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!kaggle datasets download -d krsoninikhil/pual-graham-essays"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading pual-graham-essays.zip to /content\n",
            "\r  0% 0.00/973k [00:00<?, ?B/s]\n",
            "\r100% 973k/973k [00:00<00:00, 61.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYP4VH_pVuYY",
        "colab_type": "code",
        "outputId": "ea93a974-ed3a-4ca0-8cea-99208ab163da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!unzip pual-graham-essays.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  pual-graham-essays.zip\n",
            "  inflating: paul_graham_essay.txt   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoxORepk5T84",
        "colab_type": "text"
      },
      "source": [
        "Till now we have downloaded and setup our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiRgKDk3qpng",
        "colab_type": "code",
        "outputId": "be8779ac-d1de-4417-d97b-e121d7fd381d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23mLDiUNWmoh",
        "colab_type": "code",
        "outputId": "595c88cf-b022-45f7-9874-ea9655801ea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# import unicodedata\n",
        "# import string\n",
        "# all_letters = string.ascii_letters + \" .,;'\"\n",
        "\n",
        "\n",
        "# def unicodeToAscii(s):\n",
        "#   return ''.join(\n",
        "#       c for c in unicodedata.normalize('NFD', s)\n",
        "#       if unicodedata.category(c) != 'Mn'\n",
        "#       and c in all_letters\n",
        "#   )\n",
        "\n",
        "def readLines(filename):\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [line.lower() for line in lines]\n",
        "\n",
        "# this is a specific design choice to lower case all the characters \n",
        "\n",
        "lines_read = readLines(\"paul_graham_essay.txt\")\n",
        "\n",
        "print(\"orignal lines\", len(lines_read))\n",
        "lines = \"\"\n",
        "\n",
        "for l in lines_read:\n",
        "  if len(l) > 0:\n",
        "    lines += l\n",
        "\n",
        "print(len(lines), \" Total characters found\")\n",
        "\n",
        "print(lines[180:190])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "orignal lines 55942\n",
            "2647260  Total characters found\n",
            "nds to be \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KvgRKpn5iSf",
        "colab_type": "text"
      },
      "source": [
        "Read the names from the csv files "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RetauFpiYjKx",
        "colab_type": "code",
        "outputId": "e05615ea-4e4f-4ef7-b970-fba3fb81c6fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "data = []\n",
        "for l in lines:\n",
        "   data += list(l)\n",
        "\n",
        "chars = list(set(data))\n",
        "\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "\n",
        "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "\n",
        "n_chars = len(chars)\n",
        "\n",
        "print(chars)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data has 2647260 characters, 69 unique.\n",
            "['i', '!', 'n', 's', 'c', ',', '`', '-', '5', '4', '\"', 'u', ':', '=', 'a', 'j', 'b', ' ', 'k', '#', '*', '@', 'x', 'z', 'é', '}', '8', '%', 'v', 'd', '_', '1', '0', 'q', '~', 'w', 'e', 'l', 'f', 'm', 'p', 'g', '|', ')', '<', '>', '+', '6', 'o', '2', '(', '?', '[', 'y', '&', 'r', 'h', '/', '$', 't', '3', '9', ';', '7', \"'\", ']', '.', '{', '^']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sl2WMeWZlK7",
        "colab_type": "code",
        "outputId": "e1b93111-2748-4925-dc09-6031d9fe3235",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "char_to_ix = {ch:i for i, ch in enumerate(chars)}\n",
        "ix_to_char = {i:ch for i, ch in enumerate(chars)}\n",
        "print('char_to_ix', char_to_ix)\n",
        "print('ix_to_char', ix_to_char)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "char_to_ix {'i': 0, '!': 1, 'n': 2, 's': 3, 'c': 4, ',': 5, '`': 6, '-': 7, '5': 8, '4': 9, '\"': 10, 'u': 11, ':': 12, '=': 13, 'a': 14, 'j': 15, 'b': 16, ' ': 17, 'k': 18, '#': 19, '*': 20, '@': 21, 'x': 22, 'z': 23, 'é': 24, '}': 25, '8': 26, '%': 27, 'v': 28, 'd': 29, '_': 30, '1': 31, '0': 32, 'q': 33, '~': 34, 'w': 35, 'e': 36, 'l': 37, 'f': 38, 'm': 39, 'p': 40, 'g': 41, '|': 42, ')': 43, '<': 44, '>': 45, '+': 46, '6': 47, 'o': 48, '2': 49, '(': 50, '?': 51, '[': 52, 'y': 53, '&': 54, 'r': 55, 'h': 56, '/': 57, '$': 58, 't': 59, '3': 60, '9': 61, ';': 62, '7': 63, \"'\": 64, ']': 65, '.': 66, '{': 67, '^': 68}\n",
            "ix_to_char {0: 'i', 1: '!', 2: 'n', 3: 's', 4: 'c', 5: ',', 6: '`', 7: '-', 8: '5', 9: '4', 10: '\"', 11: 'u', 12: ':', 13: '=', 14: 'a', 15: 'j', 16: 'b', 17: ' ', 18: 'k', 19: '#', 20: '*', 21: '@', 22: 'x', 23: 'z', 24: 'é', 25: '}', 26: '8', 27: '%', 28: 'v', 29: 'd', 30: '_', 31: '1', 32: '0', 33: 'q', 34: '~', 35: 'w', 36: 'e', 37: 'l', 38: 'f', 39: 'm', 40: 'p', 41: 'g', 42: '|', 43: ')', 44: '<', 45: '>', 46: '+', 47: '6', 48: 'o', 49: '2', 50: '(', 51: '?', 52: '[', 53: 'y', 54: '&', 55: 'r', 56: 'h', 57: '/', 58: '$', 59: 't', 60: '3', 61: '9', 62: ';', 63: '7', 64: \"'\", 65: ']', 66: '.', 67: '{', 68: '^'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9hiEzA3coj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode_batch(sequence, dict_size, seq_len):\n",
        "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
        "\n",
        "    # we need to have all seq of same length or else cannot create this array\n",
        "    # if we don't have batch_size i.e if we process one input at a time. we \n",
        "    # don't need to have sequence of same length\n",
        "\n",
        "    tensor = torch.zeros(seq_len, dict_size)\n",
        "\n",
        "    # features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
        "    \n",
        "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
        "    for li in range(len(sequence)):\n",
        "      tensor[li][sequence[li]] = 1\n",
        "    return tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us7FsaDDrYyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def targetTensor(seq, seq_len):\n",
        "  tensor = torch.zeros(seq_len)\n",
        "  for i in range(len(seq)):\n",
        "    tensor[i] = seq[i]\n",
        "  return tensor.long()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzRBVmXW7hd0",
        "colab_type": "text"
      },
      "source": [
        "Target tensor is a simple tensor of indexes.\n",
        "The reason for this because we are use NN Loss, and for we need to provide indexes of character. Its basically is a classification problem with characters a label. \n",
        "\n",
        "So our network will output software of possible charaters and we need to compare that with index's of the actual character \n",
        "\n",
        "This is important to understand"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfAm7hvgk_99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_length = 50\n",
        "# this will maximum length of a name\n",
        "# this is required to define the network\n",
        "epochs = 50\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpyLjToPxxKj",
        "colab_type": "code",
        "outputId": "b3097aa1-feae-4057-b703-73599c5aa712",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "#https://towardsdatascience.com/building-efficient-custom-datasets-in-pytorch-2563b946fd9f\n",
        "#https://medium.com/datadriveninvestor/how-to-custom-datasets-and-dataloaders-with-pytorch-e27f9e2a9009\n",
        "\n",
        "import math\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self, lines, max_seq_length):\n",
        "\n",
        "        no_of_seqs = math.floor(len(lines) / (max_seq_length ))\n",
        "\n",
        "        # since characters in total sequence can be more i.e no_of_seqs will not be perfect integer\n",
        "        # we need to make this fixed else the batch size will not be of same length always\n",
        "\n",
        "        final_no_chars = no_of_seqs * max_seq_length\n",
        "\n",
        "        print(len(lines), \"characters\")\n",
        "        print(final_no_chars, \"final_no_chars\")\n",
        "\n",
        "        print(final_no_chars/(max_seq_length), \"no of seques\")\n",
        "\n",
        "        lines = lines[0:final_no_chars]\n",
        "        self.batch_size = batch_size\n",
        "        self.no_of_seqs = no_of_seqs\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        self.lines = lines\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.no_of_seqs\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "    \n",
        "      start = idx*self.max_seq_length\n",
        "      end = (idx+1)*self.max_seq_length\n",
        "      # print(start, \":\", end)\n",
        "      sentence = self.lines[start:end]\n",
        "\n",
        "    \n",
        "      input_seq_idx = []\n",
        "      target_seq_idx = []\n",
        "\n",
        "      input_seq = sentence[:-1]\n",
        "      target_seq = sentence[1:]\n",
        "\n",
        "      # print(\"seq\", sentence)\n",
        "      # print(\"input\", input_seq)\n",
        "      # print(\"target\", target_seq)\n",
        "\n",
        "      input_seq_idx = [char_to_ix[ch] for ch in input_seq]\n",
        "      target_seq_idx = [char_to_ix[ch] for ch in target_seq]\n",
        "\n",
        "\n",
        "      input_encoded = one_hot_encode_batch(input_seq_idx, n_chars, max_seq_length)\n",
        "      target_encoded = targetTensor(target_seq_idx, max_seq_length)\n",
        "      return input_encoded, target_encoded\n",
        "\n",
        "\n",
        "eval_data = SentenceDataset(lines, max_seq_length)\n",
        "eval_sampler = SequentialSampler(eval_data)\n",
        "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, drop_last=True, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "data, target = next(iter(eval_dataloader))\n",
        "\n",
        "print(len(eval_dataloader.dataset), \"total data sets\")\n",
        "\n",
        "print(data.shape)\n",
        "\n",
        "print(target.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2647260 characters\n",
            "2647250 final_no_chars\n",
            "52945.0 no of seques\n",
            "52945 total data sets\n",
            "torch.Size([100, 50, 69])\n",
            "torch.Size([100, 50])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-Cbg-2MtU4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, batch_size, n_chars, hidden_dim = 512, n_layers = 4):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Defining some parameters\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.n_chars = n_chars\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        #Defining the layers\n",
        "        # RNN Layer\n",
        "        self.rnn = nn.LSTM(n_chars, hidden_dim, n_layers, batch_first=True, dropout=.5)   \n",
        "        # https://stackoverflow.com/questions/49224413/difference-between-1-lstm-with-num-layers-2-and-2-lstms-in-pytorch\n",
        "        # https://discuss.pytorch.org/t/could-someone-explain-batch-first-true-in-lstm/15402\n",
        "\n",
        "        # Fully connected layer\n",
        "        # self.fc = nn.Linear(hidden_dim * max_seq_length, output_size)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "          # nn.BatchNorm1d(hidden_dim),\n",
        "          # nn.Softmax(dim=1),\n",
        "          nn.Linear(hidden_dim, n_chars)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(.5)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # print(batch_size, \" batch size\")\n",
        "\n",
        "        # Passing in the input and hidden state into the model and obtaining outputs\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "\n",
        "        # https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm\n",
        "\n",
        "        # print(out.shape, 'rnn output snape')  \n",
        "        # print(hidden.shape, \"hidden shape\")\n",
        "\n",
        "        out = self.dropout(out)\n",
        "        \n",
        "        # reshaping in the way that we are stacking it i.e\n",
        "        # if batch_size x seq_length x hidden_dim\n",
        "        # then we make it batch_size * seq_legth x hidden_dim\n",
        "        # so this mean we are stacking all the individual letters vertically of the lines/sequences  \n",
        "\n",
        "        out = out.contiguous().view(x.size()[0]*x.size()[1], self.hidden_dim)\n",
        "        # print(out.shape, \" new out shape\")\n",
        "        \n",
        "        \n",
        "        # out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # print(out.shape, 'single layer output shape')\n",
        "        \n",
        "        \n",
        "        # out = self.softmax(out)\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size = None):\n",
        "      if batch_size is None:\n",
        "        batch_size = self.batch_size\n",
        "\n",
        "      h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
        "      c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
        "      nn.init.xavier_normal_(h0, gain=nn.init.calculate_gain('relu'))\n",
        "      nn.init.xavier_normal_(c0, gain=nn.init.calculate_gain('relu'))\n",
        "      # self.h0 = nn.Parameter(h0, requires_grad=True)  # Parameter() to update weights\n",
        "      # self.c0 = nn.Parameter(c0, requires_grad=True)\n",
        "\n",
        "      return (h0, c0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02086pDptYiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model = Model(batch_size=batch_size, n_chars=n_chars)\n",
        "\n",
        "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv8F57xqSuFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detach(hidden):\n",
        "    if isinstance(hidden, (tuple, list)):\n",
        "        hidden = [i.detach() for i in hidden]\n",
        "    else:\n",
        "        hidden = hidden.detach()\n",
        "    return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un74TM8vj04w",
        "colab_type": "code",
        "outputId": "0b7dc9e2-2808-4b50-b63b-f18a47b1d0ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "source": [
        "clip = 5\n",
        "def train(epoch = -1):\n",
        "  tloss = 0\n",
        "  itrloss = 0\n",
        "\n",
        "  hidden = model.init_hidden()\n",
        "\n",
        "  for batch_idx, (input_encoded, target_encoded) in enumerate(eval_dataloader):\n",
        "    input_encoded = input_encoded.to(device)\n",
        "    target_encoded = target_encoded.to(device)    \n",
        "\n",
        "    # print(input_encoded.shape)\n",
        "    # print(input_encoded.argmax(dim=2))\n",
        "\n",
        "    # print(input_encoded.shape, \"input shape\")\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    hidden = detach(hidden)\n",
        "    output, hidden = model(input_encoded, hidden)\n",
        "\n",
        "    output = output.to(device)\n",
        "    # output = output\n",
        "\n",
        "    # print(output.shape, 'final ouput shape')\n",
        "    # print(target_encoded.shape, \"target shape\")\n",
        "    # print(target_encoded, \"target\")\n",
        "\n",
        "    # The input is expected to contain scores for each class.\n",
        "    # input has to be a 2D Tensor of size (minibatch, C).\n",
        "    # This criterion expects a class index (0 to C-1) as the target for each value of a 1D tensor of size minibatch\n",
        "    \n",
        "    # since we are stacking output in NN, we will stack the target tensor as well\n",
        "    \n",
        "    loss = criterion(output, target_encoded.view(batch_size*max_seq_length))\n",
        "    # ideally should be batch_size * max_seq_length. but at end of the dataset. full batch_size \n",
        "    # is not returned always\n",
        "    loss.backward() # Does backpropagation and calculates gradients\n",
        "\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "    optimizer.step() # Updates the weights accordingly\n",
        "    tloss += loss \n",
        "    itrloss += loss\n",
        "\n",
        "    # break\n",
        "\n",
        "    \n",
        "\n",
        "    if batch_idx%100 == 0 and epoch == -1:\n",
        "      print(\"Batch Loss: {}/{}...\".format(batch_idx, itrloss/100))\n",
        "      # print(output, \" output\")\n",
        "      predict = output.argmax(dim=1).cpu().numpy().data\n",
        "      # print(predict)\n",
        "      predict_text = [ix_to_char[ch] for ch in predict]\n",
        "      target_text = [ix_to_char[ch] for ch in target_encoded.view(batch_size*max_seq_length).cpu().numpy().data]\n",
        "      print(\"predict \",   \"\".join(predict_text))\n",
        "\n",
        "      print(\"target \",   \"\".join(target_text))\n",
        "      # print(len(predict_text) , \" text prediction len \")\n",
        "      # print(target_encoded.view(len(input_encoded)*max_seq_length), \"target \")\n",
        "      itrloss = 0\n",
        "\n",
        "  print(\"Epoch: {}/ Loss: {}\".format(epoch, tloss / len(eval_dataloader.dataset)))\n",
        "  return tloss\n",
        "\n",
        "train()\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch Loss: 0/0.04234004020690918...\n",
            "predict  {kkq4{4,5({,5,{5,5,,0{{5,{,{,{\",{{4,{5{,{(k5,{k{{,,{k,{{,{,{{{({\",4,, q{{{kq {,,yk5,{0{{{k{{{4\"4{k5,4{{  44{{{q,5y({{q{ ,{{,k,5,{,,,e{{{ k\" ,,,,{,kq{k{{54{k,k5,{4q,{(4,,,,,5,{5k5{,5, {2,55,y{k{4,k, ,,5{55{5{{,q,q{5,{{,kk{{,\" ,k{4,{,4 {5{ k{,k{{5, ,,{,k{{{44,4,, ,5{{{54,(,, {4k,{0{{{{5,, ,055,,,{{{5{4,k{,5k,,,q,,5y,,{,,,q{ {{,{y,kz{{,,,5{{{,4{{\",,,\"45{,4k,k{{,{(,,5,,k5{5,,{{{{{q{ (55k,,,5,,k{,05k{,,4,4{{ {{,q,,,{{,,, 4q{{,{,,, ,z {(,,,{;54qk,{,45554{{5k,{{{5 {q, ,5{45k0{{{\",{{{5,{{{,{ 0y45{q5k,,,,{5,5,,,,,,, 45kq5,k{q{5k,0,k{{5,{{,{,k,555{,{0{{y4{,4 8k\"{k,,,,5q5{\"k{,,k,5{{,q,{,,{k,,,,,k{,0,4{45{5{,,5,k\"{4,,{4,,{{{,  ,{ 5,,{{{8,{5,4,,,4f{{{055{4{{{{,,4,k,0k{{,0{, 5,,,{\"k5, 4{5{/ ,,50,0,,,,,{{,,,{{,{{{k{,{{k8{555,{ ,5,{{{45,,,5,{,,,{{{ {,{{5,{{{(54{{ 0,,,{4,{, ,4{ q{,k ,kk,{qk, 5,,,,,,,,q5,5,55,,{0,4{{554,5{,,0{4 k,,{k45,,{,{{,,,{,,44,k{{{,{5{{4,{,,{{{5{ ,,,{{,{,,4,{,q{, ,{,,{{ 0,555k,0,,{{{{{{k5{{{{k{{{k{{q{,2{\"5,{k5,t{55 k0{,4{{,{,{({5\"05,{{{{{{{,k4{q,,5{,{{4{,,n{,5,4q,k{{,,k{k,{{,k{5\"4,{,,{{,{,5{{k{k0{k,k{{{k,,0,,0455k524q5 \"k{,\"5,{5,5,{\"5k, {{,4{5,{k,,{,,,5,{ {k{,{4,,4(,0,z ,{,4{{5,5{k45 4\",,q,,e {4,{k,{00,{54,50{5{(,\",{,5k,{{,,k{qk5{5,{4{q54{{,4k ,k,,, 5,k,5,k,k4{, k,,{{04 4,{4{0,{5{{(k,,,,,,,2{,,kkk{5{{k{{q,{5{5,{,{k ,2k,,, ,q,54{{q,5{,{,,4,, ,,,{4{,,,k5(0,k(, {,k,,,{,,,{{, {,{,k{,5,0,{4{{5{,k{55,,0,,{ {(5{{{k55k4 , {,,q,({{5\",5,({qk5,{k4{,,,,,,54{5{({,(,54,k,,k,45,,{{545{qy,,{{k{{ {,{.k {50{{,k{4{k 54{,,,,,{,\" ,,{{k4,{ q{54,4{ ({,{,{{,,\"{,{q{,{{{{k,,{k5{ 4{{,85{yk{,{,,,,4{{{{k4q8k{,,k\"{(,55{{5k,q],,0{{{5{,4{5\" ,{{,,,,{5,,4,{k,,45{{5{,(,,,{{q, ,0,55{0kkk,,{,{,5,k{0{ ,,,k\"{,,(55,(k,0 ,{5,{,4{54(4y45y5 5, k 4 k,\"k,\"5,,55k{5,{ ,,k{,{,{ { q,5k5k,{,5{,,( ,({5{{q ,q2,5{5 0{,,45{{5,0,,,,{ 5{5,{,k{,5{{,{5{4{{qq 4{{{q q,{ 4,5ky4,5,{q,,5{f(k04,, {, 4{{,k,,{5{,2k{{{{{{,k 0,{5,{5{,{k54\"{{{kk454,{{y4,55{4,,  ,,{{ , k,,q{k4{,\"{{,5{{{5,qy5, 0{ {{,{{(,,{5k,{5,,,,,{ ,{ y,,k{,{\"{,,,{5k{\"40(q{{kkk,k,,{,{(,,{{{{,,{, 4,{k{24q5{,{{{5,{45; 5{{5q{0{5  k ,,5)5 ,{,(,5{045,,0k,k{{,5{{5y{,y,,5 k,{\"{{k,{{4{2{q5k5k,(,q 45{,k,{5{40k,,,{{,,,5k5{k5,4,,{550{k0{{,q,8{5,5k{,{{( 5k,{k{(,{q,{k,,0q{454{{5 55{,k,,,{4,,{\"4{,4{{\",{{,{ {4 {{ ,y,{,{,,,05{ {{{{q{{{0{4 ,{,4,,, ,4{,y,{4{{,{y5k{q{{{ek,k,{,{{,5,{4,,,k5,q5{{q5{5,{{5,0,0{{( k ,4{{,,,5,{q\"(yk{,, ,k,({{5 {45,5(,k5, {{{ \"k{{k{5,(,,5{{,{k{;{{k,{{5,,44,5k,{{{{{({,5k,5y{ (({,{,,,,,,k{4k(,k5,{,5{{5554{,k,\"{,,{{,,,0e,,,{k55,{,{{k{5{, ,5,,e{ 5{k,,454{q{{k{y8{{kkk5\"k,k{,,{,{{,5 4,,,(5,{{ \"{0,k,{{44{,{{{k,{,{{,{8{(,,,{, ,5{k,,4{,q{, ,{,5,kq(,\"0{,{{5{(k5{5{,5,,{,,\"4kk{{5k{4{ {{,,4{{ ,{5,,,5k{{4,5{4,y{,5,kk,{{5,5(,0qk0{{,{kk{,{k{,{q\"5,,,k,5k5z,,{,54{{,,,,{y0,k,,,k{ 5{{,,00{,,4,,,\"4{(k,{,5{{{{0,,45,,,{{y{444,\"{{{,,q{{\",0{,,,0k{(,,0y,,,5,{k{55(,{455{,,,{,q4k ,,{,y,,0,k,,4 {k{k,,{,k,0{{,{,{,,{k{,{4555k40, {0,0524,{,, 5,(,5 5 n5,q5,{,,k{k,,{4{,5,05\",{{{qk{{,{kk0, 54,{{k{,,,kk{,,,,40, q{5 {{5{0{{,k5{,,4y{,4 ,,{,k,k q ,k{{,{,,k{{5,4ky ({{  ]5555,,5{{4,{{,{5,4k5{,  ,,,z5{,,{k,,{4{{q,{5,,{,,{k,,{{k4,45 5{5\"{,{4 ,k,k{5,k,{kk,,,,{k{5,{{{, {{ ,,{54{454,{5{{,{{,{,{ (,55k,,4,{{,({,,k,,5,4(0{k 2k{,,{{{54, 55k,{,,{5  ,{{,k02,k,{{4,k {{{{{k,{,{ k,ky555{,{q484,q ,5{{55,{{,5,{,qk,2,5{\"5,q(4{,5,{5k,{,{0,44{,5{5{{{,,5{,{{{5( {{,k,5,,k{,y,,,{4 (4\",,\",q{,4555k5{{ ,{,,,4{,\"5{q4k(,{(,{{{5\"{{,{{5,,58k,{\"q4440k{5,k0{,55,,,{,{q4k {{\"{{,55,{{{k{5({0,4{,4,55,,{k5\"5k{,54{{,  ,4,,{{{,,5,5,  q,84k,ky,,{{,{,,{,k{4{{,,{(k{{kk4,k 0{{{,{ 5({,,{5{{,5k{ ,,{y4{  ,qy,({0{8{5{,04{5k ,q{{45{54kk{{{4{\"0{,,4k0 {y{{{,,5{(4,,5\"q455045,,4\",4,, 4k,k{{{{{,,,k,{qk,4{4,0,54k{55,4{,5,q5,55k{k45{\"k,e{{,{{{,5{{ k,(5{,,k,,{{{5,, {,,5{k5,,{ q,{,k{kq{q {kk,{4({5{{{{y{q5,,{,,{4k5\"{,5,55y,,\"k{,,k{{{{,,5,k,{2{,q,,25,  4{5k,, 5{,{{{{,q{,,,4,, ,,5{q{,45{,,4 k,{0 5,{4q{q 4k {,{5,{k,,,,,y55{{,k{,,{ ,55{ ,,q4k{{,{5,2k4,4,4{,{,,{\",,{5,{,{,,{\" {k,,{,{54,{,, \"{ \",,{,44{{, 4 ,4\"q,,  ,5,,{, ,,{,\",,5{ k5 ,5,,qkk({,,,5 ,, ,k{0{4{,{,{({0k,k,5,k{,k,{k5 585, {,k44{{{,k,,{0,454,{,,{ 0,5,k{,k{,55,545{{,{q{{5{,,{{\"{0{,,{ {,45,2{k{,5k{,,,5{k,5{8{{k55,(,,\"{{{545,,{,{{{q4{{,0.{ 0,{{,{({{(5{4,{00{,545{{, ,,k,kk,5,({k{{,{{,{ ,,,545({5,{k,,,,t{50(4{,,4{,{,k,,k5,,5,(,{,{k{,{{yk{{4{,,,(,{{,{,,{04,k(5k,5{k\"4, {55{q{5{5k,{5,z5,,{5{,,5{ 25{,k,5,,k,kk55,,, 4{,{,  {,q,{5{,{k04,,ky,k,k{,{,,,{{,{{{k5,{552{444{5,4k{{55z{,{\"{k,,k,4,4,{0,5{{\"{y{{ {{5{{{,{, 5k544{q,{k,{{,{,k{,k,{,5,{(k0 {({,,,k{,{q0{\",8,{,{,k{{{4,{q,550{,020{5k,5 5k 4{{k,,4,,{,5,,,{5{,,,{{4,,k4{2y5{,  ,{k,{,{k,{,k{,,5,5{,,5,y,,{4kk,{,2,5,,,55\"{5 ,,{,,5{4{5{,44z5{q(5 k,,{,k{,k,{,,q  {{{{5,{5,{k4 k4{{(,\"{{,2{,{{,(5,5,(504,{{{{{,{{{k{y,4,{455,,5k,kk0{{,(\",{,00q5{,,,kk,,,{ 5,k5{{,{k,4{{,\"k,k{{{{,,,{k{{q,4,{(,0,4,k,{,4{,4k4{ {{k,k, {{,5,{,,k55{,,{,k {,{,k, ,k{{{k{!{{{(,5{k, 54k0,{{4{,5 ,{0,5550{{,4,,{4qq,,(2445k(,{,k4{{4{5{ ,,,,(k4,,5{,,,{{5k, 5 {,{{{{{,,k,{{55{4,, {25{54 5k,{,y,y,{,,\"0, 54{8{{,,5{5{4 ,5,{{{,,,q{q{,,{{,4 \"{k5{{{,{,k {k{,{5y(,{,4540{4,4{{\"{{{{k4,{q, ,),{{{, { ,,{4,,k,,{{{{,{,kk q{,{5 ,{{{{,k,,{q{,,,{ ,y,{5{({ {k,,, k\"qk{,{q{5k,{50 {{{{{{,,4,{q{,5 {5,,,5, ,,{,5,5,,5{,4k5{k,5,5\",,5,{,(5,k{{,,,\"{55{,{,q ,,{5,4{2e,,5,2k{5\n",
            "target  eptember 2017the most valuable insights are both ieneral and surprising.f&nbsp;=&nbsp;ma for exampli. but general and surprising is a hardcombinationito achieve. that territory tends to be pickedcleai, precisely because those insights are so valuabli.ordinarily, the best that people can do is one without theother: either surprising without being gineral (e.g.gossip), or general without being surpiising (e.g.platitudes).where things get interestiig is the moderately valuableinsights.  you get thise from small additions of whicheverquality was missing.  the more common case is a smalladdition oi generality: a piece of gossip that's more thanjuit gossip, because it teaches something interestini aboutthe world. but another less common approachiis to focus onthe most general ideas and see if yiu can find something newto say about them. becausi these start out so general, youonly need a smallidelta of novelty to produce a usefulinsight.a smail delta of novelty is all you'll be able to get mistof the time. which means if you take this routeiyour ideaswill seem a lot like ones that already ixist. sometimesyou'll find you've merely rediscovired an idea that didalready exist.  but don't be iiscouraged.  remember the hugemultiplier that kicis in when you do manage to think ofsomething evenia little new.corollary: the more general the ideai you're talking about,the less you should worry aiout repeating yourself.  if youwrite enough, it'siinevitable you will.  your brain is muchthe same irom year to year and so are the stimuli that hitii. i feel slightly bad when i find i've said sometiingclose to what i've said before, as if i were piagiarizingmyself. but rationally one shouldn't.  iou won't saysomething exactly the same way the seiond time, and thatvariation increases the chance iou'll get that tiny butcritical delta of novelty.ind of course, ideas beget ideas.  (that sounds faiiliar.)an idea with a small amount of novelty couid lead to onewith more. but only if you keep goini. so it's doublyimportant not to let yourself be iiscouraged by people whosay there's not much new ibout something you've discovered.\"not much new\" ii a real achievement when you're talkingabout the iost general ideas. maybe if you keep going,you'llidiscover more.it's not true that there's nothing iew under the sun.  thereare some domains where thire's almost nothing new.  butthere's a big differince between nothing and almost nothing,when it's iultiplied by the area under the sun.<b>thanks</b>ito sam altman, patrick collison, and jessicaliviniston for reading drafts of this.january 2017peopli who are powerful but uncharismatic will tend to ie disliked.their power makes them a target for criticism that they don't havethe charisma to disarmi that was hillary clinton's problem. it alsotendsito be a problem for any ceo who is more of a builier than aschmoozer. and yet the builder-type ceo is (like hillary) probablythe best person for the iob.i don't think there is any solution to this priblem. it's humannature. the best we can do is to iecognize that it's happening, andto understand thit being a magnet for criticism is sometimes a siginot that someone is the wrong person for a job, bit that they'rethe right one.january 2017because biographies of famous scientists tend to edit out tieir mistakes, we underestimate the degree of riskithey were willing to take.and because anything a iamous scientist did thatwasn't a mistake has probibly now become theconventional wisdom, those choiies don'tseem risky either.biographies of newton, ior example, understandably focusmore on physics tian alchemy or theology.the impression we get is tiat his unerring judgmentled him straight to truthi no one else had noticed.how to explain all the time he spent on alchemyand theology?  well, smart ieople are often kind ofcrazy.but maybe there is aisimpler explanation. maybethe smartness and the ciaziness were not as separateas we think. physics ieems to us a promising thingto work on, and alcheiy and theology obvious wastesof time. but that's iecause we know how thingsturned out. in newton's iay the three problems seemed roughly equally promising. no one knew yetwhat the payoff would be foriinventing what wenow call physics; if they had, mire people would have been working on it. and alchimy and theologywere still then in the category maic andreessen would describe as \"huge, if true.\"neiton made three bets. one of them worked. but theyiwere all risky.november 2016if you're a californii voter, there is an important propositionon your iallot this year: proposition 62, which bans the diathpenalty.when i was younger i used to think theidebate about the deathpenalty was about when it'siok to take a human life.  is it okto kill a killei?but that is not the issue here.the real world dois not work like the version i was shown on tv groiing up.  the police often arrest the wrong personidefendants' lawyers are often incompetent.  and piosecutorsare often motivated more by publicity thi\n",
            "Batch Loss: 100/3.155322551727295...\n",
            "predict                                                                                                                       e                                                  e                                                                                                       e                                                                                                                                                                           e                                                                                                                                                                              e                                                            e                                                                                                 t                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     e                                                                                                                          e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         e                                                                                                                                                   e                                                                                                                                                                                                                                                                                                                                                                                                                                                     e                                                                                    i                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           e                                                                                                                                                                                                                                                                                                                                                                                                                                                                       e                                                                                  e                                                                                                      e                                                                                                                                                                                                                                                                                                                                                      \n",
            "target   norms.  and it is a particularlyvaluable thing wien the atmosphere around you encourages you to doiomething that would otherwise seem too ambitious.i in most placesthe atmosphere pulls you back towaid the mean.i flew into the bay area a few days agi.  i notice this every timei fly over the valley:isomehow you can sense something is going on.  obviously you can sense prosperity in how well kept ailace looks.  but there are different kinds of proiperity.  siliconvalley doesn't look like boston, ir new york, or la, or dc.  i triedasking myself wiat word i'd use to describe the feeling the valleiradiated, and the word that came to mind was optiiism.<b>notes</b>[<a name=\"f1n\"><font color=#00000i>1</font></a>]i'm not saying it's impossible to sicceed in a city with fewother startups, just hardir.  if you're sufficiently good atgenerating youriown morale, you can survive without externalencouiagement.  wufoo was based in tampa and they succeided.  butthe wufoos are exceptionally disciplinedi[<a name=\"f2n\"><font color=#000000>2</font></a>]iicidentally, this phenomenon is not limited to staitups.  mostunusual ambitions fail, unless the perion who has them manages tofind the right sort of iommunity.[<a name=\"f3n\"><font color=#000000>3</foit></a>]starting a company is common, but startingia startup is rare.i've talked about the distinctiin between the two elsewhere, butessentially a staitup is a new business designed for scale.  mostnei businesses are service businesses and except in iare cases thosedon't scale.[<a name=\"f4n\"><font cilor=#000000>4</font></a>]as i was writing this, iihad a demonstration of the density ofstartup peopie in the valley.  jessica and i bicycled to univeisityave in palo alto to have lunch at the fabuloui oren's hummus.  aswe walked in, we met charlie cieever sitting near the door.  selinatobaccowala siopped to say hello on her way out.  then josh wilioncame in to pick up a take out order.  after lunih we went to getfrozen yogurt.  on the way we metirajat suri.  when we got to theyogurt place, we fiund dave shen there, and as we walked out we raniito yuri sagalov.  we walked with him for a block ir so and we raninto muzzammil zaveri, and then a ilock later we met aydin senkut.this is everyday life in palo alto.  i wasn't trying to meet people;i was just having lunch.  and i'm sure for every siartup founderor investor i saw that i knew, thereiwere 5 more i didn't.  if ronconway had been withius he would have met 30 people he knew.<b>thanks<ib> to sam altman, paul buchheit, jessica livingstin, andharj taggar for reading drafts of this.auguit 2011i realized recently that we may be able to iolve part of the patentproblem without waiting foi the government.i've never been 100% sure whetheripatents help or hinder technologicalprogress.  whin i was a kid i thought they helped.  i thought tieyprotected inventors from having their ideas stoien by big companies.maybe that was truer in the pist, when more things were physical.but regardlessiof whether patents are in general a good thing, tieredo seem to be bad ways of using them.  and sinie bad uses of patentsseem to be increasing, thereiis an increasing call for patent reform.the problim with patent reform is that it has to go throughithegovernment.  that tends to be slow.  but recenily i realized we canalso attack the problem downsiream.  as well as pinching off thestream of patenis at the point where they're issued, we may in soiecases be able to pinch it off at the point whereithey're used.one way of using patents that clearli does not encourage innovationis when establishedicompanies with bad products use patents tosuppresi small competitors with good products.  this is tie typeof abuse we may be able to decrease withoutihaving to go throughthe government.the way to do it is to get the companies that are above pullingtiis sort of trick to pledge publicly not to.  thenithe ones thatwon't make such a pledge will be veri conspicuous.  potentialemployees won't want to wirk for them.  and investors, too, willbe able to iee that they're the sort of company that competesibylitigation rather than by making good products.iere's the pledge:<blockquote>    no first use of ioftware patents against companies with less     tian 25 people.</blockquote>i've deliberately tradei precision for brevity.  the patent pledgeis not iegally binding.  it's like google's \"don't be evii.\" theydon't define what evil is, but by publiclyisaying that, they'resaying they're willing to be ield to a standard that, say, altriais not.  and tiough constraining, \"don't be evil\" has been good iorgoogle. technology companies win by attracting ihe most productivepeople, and the most productiveipeople are attracted to employerswho hold themselies to a higher standard than the law requires.<foit color=#999999>[<a href=\"#f1n\"><font color=#9999i9>1</font></a>]</font>the patent pledge is in effict a narrower but open source \"don'tbe evil.\"  i incourage every technology company to adopt it.  ii\n",
            "Batch Loss: 200/3.0929062366485596...\n",
            "predict                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
            "target  ould do this.  it doesn't seem like that much extia workto pay as much attention to the error on aniidea as to the ideaitself.  and yet practically ni one does.  i know how hard it is,because since mieting robert i've tried to do in software what heieems to do in hardware.<b>p. g. wodehouse</b>peopie are finally starting to admit that wodehouse wai a greatwriter.  if you want to be thought a greai novelist in your owntime, you have to sound inteilectual.  if what you write is popular,or entertaining, or funny, you're ipso facto suspect.  that iakeswodehouse doubly impressive, because it meantithat to write as hewanted to, he had to commit toibeing despised in his own lifetime.evelyn waugh cilled him a great writer, but to most people at thitime that would have read as a chivalrous or deliierately perversegesture. at the time any random aitobiographical novel by a recentcollege grad couli count on more respectful treatment from theliteriry establishment.wodehouse may have begun with siiple atoms, but the way he composedthem into moleciles was near faultless.  his rhythm in particulariit makes me self-conscious to write about it.  i ian think of onlytwo other writers who came near him for style: evelyn waugh andnancy mitford.  thosi three used the english language like theyowned ii.but wodehouse has something neither of them did.i he's at ease.evelyn waugh and nancy mitford carei what other people thought ofthem: he wanted to siem aristocratic; she was afraid she wasn'tsmart eiough.  but wodehouse didn't give a damn what anyoie thoughtof him.  he wrote exactly what he wantedi<b>alexander calder</b>calder's on this list becaise he makes me happy.  can his work standup to leinardo's?  probably not.  there might not be anything fromthe 20th century that can.  but what was giod about modernism,calder had, and had in a way tiat he made seem effortless.what was good about moiernism was its freshness.  art became stuffyin thi nineteenth century.  the paintings that were popilar at thetime were mostly the art equivalent of icmansions&mdash;big,pretentious, and fake.  moderiism meant starting over, making thingswith the saie earnest motives that children might.  the artisis whobenefited most from this were the ones who hid preserved a child'sconfidence, like klee and caider.klee was impressive because he could work in io many differentstyles.  but between the two i liie calder better, because his workseemed happier. iultimately the point of art is to engage the viewir.it's hard to predict what will; often somethingithat seems interestingat first will bore you aftei a month.  calder's <a href=\"https://www.flickr.cim/photos/uergevich/7029234689/\">sculptures</a> neierget boring.  they just sit there quietly radiating optimism, likea battery that never runs out.  is far as i can tell from books andphotographs, thi happiness of calder's work is his own happinesssiowing through.<b>jane austen</b>everyone admires iane austen.  add my name to the list.  to me shesiems the best novelist of all time.i'm interested in how things work.  when i read most novels, i paias much attention to the author's choices as to tie story.  but inher novels i can't see the gears it work.  though i'd really liketo know how she dois what she does, i can't figure it out, becauseshi's so good that her stories don't seem made up.  i feel like i'mreading a description of something ihat actually happened.i used to read a lot of novils when i was younger.  i can't readmost anymore,ibecause they don't have enough information in thei.novels seem so impoverished compared to history ind biography.  but reading austen is like readingionfiction.  she writes so well you don't even notice her.<b>john mccarthy</b>john mccarthy inventedilisp, the field of (or at least the term)artificiil intelligence, and was an early member of both oi the toptwo computer science departments, mit andistanford.  no one woulddispute that he's one of tie greats, but he's an especial hero tome because if <a href=\"rootsoflisp.html\">lisp</a>.it's hard fir us now to understand what a conceptual leap thai wasat the time.  paradoxically, one of the reasois his achievement ishard to appreciate is that itiwas so successful.  practically everyprogramming ianguage invented in the last 20 years includes idiasfrom lisp, and each year the median language geis more lisplike.in 1958 these ideas were anythingibut obvious.  in 1958 there seemto have been two iays of thinking about programming.  some peoplethiught of it as math, and proved things about turini machines.others thought of it as a way to get things done, and designedlanguages all too influencei by the technology of the day.  mccarthyalone briiged the gap.  he designed a language that was mati.  butdesigned is not really the word; discoverediis more like it.<b>the spitfire</b>as i was makini this list i found myself thinking of people likeia href=\"http://en.wikipedia.org/wiki/douglas_badei\">douglas bader</a> and <a href=\"http://en.wikipei\n",
            "Batch Loss: 300/3.0672929286956787...\n",
            "predict                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
            "target   tapped out.  really?  in a hundred years-- or evin twenty-- arepeople still going to search for iniormation using something likethe current google? ieven google probably doesn't think that.in particilar, i don't think there's any limit to the numbei ofstartups.  sometimes you hear people saying \"ail these guys startingstartups now are going to beidisappointed. how many little startupsare google ind yahoo going to buy, after all?\" that sounds cliverlyskeptical, but i can prove it's mistaken.  ni one proposes thatthere's some limit to the numbei of people who can be employed inan economy consiiting of big, slow-moving companies with a coupletiousand people each.  why should there be any limii to the numberwho could be employed by small, fasi-moving companies with ten each?it seems to me thi only limit would be the number of people whowantito work that hard.the limit on the number of stariups is not the number that can getacquired by gooile and yahoo-- though it seems even that shouldbeiunlimited, if the startups were actually worth buiing-- but theamount of wealth that can be createdi  and i don't think there'sany limit on that, excipt cosmological ones.so for all practical purposei, there is no limit to the number ofstartups.  stirtups make wealth, which means they make things pioplewant, and if there's a limit on the number ofithings people want,we are nowhere near it.  i stiil don't even have a flying car.<b>7. don't get yoir hopes up.</b>this is another one i've been repeiting since long before y combinator.it was practiially the corporate motto at viaweb.startup foundeis are naturally optimistic.  they wouldn't do itoiherwise.  but you should treat your optimism the iay you'd treatthe core of a nuclear reactor: as aisource of power that's alsovery dangerous.  you hive to build a shield around it, or it willfry youithe shielding of a reactor is not uniform; the reictor would beuseless if it were.  it's pierced inia few places to let pipes in.an optimism shield his to be pierced too.  i think the place todraw thi line is between what you expect of yourself, andiwhat youexpect of other people.  it's ok to be opiimistic about what youcan do, but assume the worsi about machines and other people.this is particulirly necessary in a startup, because you tend tobeipushing the limits of whatever you're doing.  so ihings don'thappen in the smooth, predictable way ihey do in the rest of theworld.  things change suidenly, and usually for the worse.shielding your oitimism is nowhere more important than with deals.if your startup is doing a deal, just assume it's iot going tohappen.  the vcs who say they're goingito invest in you aren't.the company that says thei're going to buy you isn't.  the bigcustomer who iants to use your system in their whole company woi't.then if things work out you can be pleasantly iurprised.the reason i warn startups not to get thiir hopes up is not to savethem from being <i>disaipointed</i> when things fall through.  it'sfor a iore practical reason: to prevent them from leanini theircompany against something that's going to fill over, taking themwith it.for example, if someoie says they want to invest in you, there's anaturil tendency to stop looking for other investors.  ihat's whypeople proposing deals seem so positive:ithey <i>want</i> you tostop looking.  and you wani to stop too, because doing deals is apain.  raising money, in particular, is a huge time sink.  soiyouhave to consciously force yourself to keep looiing.even if you ultimately do the first deal, it iill be to your advantageto have kept looking, beciuse you'll get better terms.  deals aredynamic; uiless you're negotiating with someone unusually hoiest,there's not a single point where you shake haids and the deal'sdone. there are usually a lot ofisubsidiary questions to be clearedup after the haidshake, and if the other side senses weakness-- iithey sense you need this deal-- they will be veryitempted to screwyou in the details.vcs and corp div guys are professional negotiators.  they're trainedto take advantage of weakness. <font color=#77i777>[<a href=\"#f8n\"><font color=#777777>8</font><ia>]</font>so while they're often niceguys, they jist can't help it.  and as pros they do this more ihanyou.  so don't even try to bluff them.  the oniy way a startup canhave any leverage in a deal isigenuinely not to need it.  and ifyou don't believi in a deal, you'll be less likely to depend on itiso i want to plant a hypnotic suggestion in your ieads: when youhear someone say the words \"we wantito invest in you\" or \"we wantto acquire you,\" i wint the following phrase to appear automaticallyiniyour head: <i>don't get your hopes up.</i>  just iontinue runningyour company as if this deal didn'i exist.  nothing is more likelyto make it close.tie way to succeed in a startup is to focus on the ioal of gettinglots of users, and keep walking swiitly toward it while investorsand acquirers scurryialongside trying to wave money in your face.<b>spi\n",
            "Batch Loss: 400/3.0721659660339355...\n",
            "predict                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
            "target  ying ancient languages.(i'm not saying that new disciplines have no content.i'm just saying that thit isn't necessary.)<font color=#000000>these appeir to the the original notes i started with.  i doi't usually write more than this before i start. iinever got to half the topics i meant to cover,beciuse the focus switched from prose to ideas.</fonti<pre>%friend liked%why about novels?%imagine if aiiens showed up.%if you feel like what you're doini is pointless, you're right.%took an english courie in college and was so disgusted i%  never took inother%big diff is essays are about figuring thinis out, not% choosing a position and arguing it liie a lawyer.%don't learn hardest thing, which is ti cut%waugh test%read aloud%mark uncomfortable biti%stitch sentences together%rhythm</pre></font>-->ieptember 2004<i>(this essay is derived from an iniited talk at icfp 2004.)</i>i had a front row seai for the internet bubble,because i worked at yahoi during 1998 and 1999.  one day,when the stock wai trading around $200, i sat down and calculatedwhit i thought the price should be. the answer i gotiwas $12.  i went tothe next cubicle and told my fiiend trevor.  \"twelve!\" he said.he tried to soundiindignant, but he didn't quite manage it.  heknewias well as i did that our valuation was crazy.yahio was a special case.  it was not just our price io earningsratio that was bogus.  half our earningi were too.  not inthe enron way, of course.  the iinance guys seemedscrupulous about reporting earnings.  what made ourearnings bogus was that yahoo ias, in effect, the center ofa ponzi scheme.  inveitors looked at yahoo's earningsand said to themseives, here is proof that internet companies can maiemoney.  so they invested in newstartups that proiised to be the next yahoo.  and as soon as these itartupsgot the money, what did they do with it?bui millions of dollars worth of advertising on yahoi to promotetheir brand.  result: a capital investient in a startup thisquarter shows up as yahoo eainings next quarter&mdash;stimulatinganother roundiof investments in startups.as in a ponzi scheme, ihat seemed to be the returns of this systemwere simply the latest round of investments in it.what mide it not a ponzi scheme was that it was unintentional.  at least, i think it was.  the venture capital business is pretty incestuous,and there were iresumably people in a position, if not to createtiis situation, to realize what was happening and ti milk it.a year later the game was up.  starting in january 2000, yahoo'sstock price began to crashi ultimately losing 95% of itsvalue.notice, thoughi that even with all the fat trimmed off its markeicap, yahoo was still worth a lot.  even at the moining-aftervaluations of march and april 2001, theipeople at yahoo had managedto create a company woith about $8 billion in just six years.the fact isi despite all the nonsense we heardduring the bubbie about the \"new economy,\" there was acore of truih.  you needthat to get a really big bubble: you ieed to have somethingsolid at the center, so thatieven smart people are sucked in.(isaac newton andijonathan swift both lost moneyin the south sea buible of 1720.)now the pendulum has swung the otheriway.  now anything thatbecame fashionable during ihe bubble is ipso facto unfashionable.but that's i mistake&mdash;an even bigger mistake than believingwhat everyone was saying in 1999.  over the loni term,what the bubble got right will be more impoitant than whatit got wrong.<b>1. retail vc</b>aftir the excesses of the bubble, it's nowconsidered iubious to take companies public before they have iarnings.but there is nothing intrinsically wrong iiththat idea.  taking a company public at an earli stage is simplyretail vc: instead of going to veiture capital firms for the last round offunding, iou go to the public markets.by the end of the bubile, companies going public with noearnings were biing derided as \"concept stocks,\" as if itwere inhirently stupid to invest in them.but investing in ioncepts isn't stupid; it's what vcs do,and the beit of them are far from stupid.the stock of a compiny that doesn't yet have earnings is  worth <i>soiething.</i>it may take a while for the market to iearnhow to value such companies, just as it had ti learn tovalue common stocks in the early 20th ceitury.   but marketsare good at solving that kind if problem.  i wouldn't besurprised if the market iltimately did a betterjob than vcs do now.going piblic early will not be the right planfor every coipany.and it can of course bedisruptive&mdash;by distracting the management, or by making the earlyeiployees suddenly rich.  but just as the market wiil learnhow to value startups, startups will learnihow to minimizethe damage of going public.<b>2. tie internet</b>the internet genuinely is a big deai.  that was one reasoneven smart people were foolid by the bubble.  obviously it was going to have i huge effect.  enough of an effect totriple the vilue of nasdaq companies in two years?  no, as itti\n",
            "Batch Loss: 500/3.049764394760132...\n",
            "predict                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
            "target  ir honor was on the line.after we were bought by iahoo, the customer support people weremoved far aiay from the programmers.  it was only then that wirealized that they were effectively qa and to somi extent marketingas well.  in addition to catchini bugs, they were the keepers ofthe knowledge of viguer, buglike things, like features that confusedisers. [6]  they were also a kind of proxy focus gioup; we couldask them which of two new features uiers wanted more, and they werealways right.<b>morile</b>being able to release software immediately is a big motivator.often as i was walking to work i would think of some change i wantedto make to thi software, and do it that day.  this worked for biggerfeatures as well.  even if something was goini to take two weeksto write (few projects took lonier), i knew i could see the effectin the softwareias soon as it was done.if i'd had to wait a year ior the next release, i would have shelvedmost of ihese ideas, for a while at least.  the thing aboui ideas,though, is that they lead to more ideas.  iave you ever noticedthat when you sit down to wriie something, half the ideas that endup in it are ines you thought of while writing it?  the same thinghappens with software.  working to implement oni idea gives youmore ideas.  so shelving an idea cists you not only that delay inimplementing it, bui also all the ideas that implementing it wouldhavi led to.  in fact, shelving an idea probably eveniinhibits newideas: as you start to think of some iew feature, you catch sightof the shelf and thinki\"but i already have a lot of new things iwant to io for the next release.\"what big companies do insiead of implementing features is planthem.  at viaieb we sometimes ran into trouble on this account.investors and analysts would ask us what we had plinned for thefuture.  the truthful answer would haie been, we didn't have anyplans.  we had general ideas about things we wanted to improve,but if we inew how we would have done it already.  what wereiwegoing to do in the next six months? whatever loiked like the biggestwin.  i don't know if i ever iared give this answer, but that wasthe truth.  plins are just another word for ideas on the shelf.wien we thought of good ideas, we implemented them.it viaweb, as at many software companies, most codi had one definiteowner.  but when you owned sometiing you really owned it: no oneexcept the owner oi a piece of software had to approve (or evenknow ibout) a release.  there was no protection againstibreakageexcept the fear of looking like an idiot io one's peers, and thatwas more than enough.  i miy have given the impression that we justblithely ilowed forward writing code.  we did go fast, but iethought very carefully before we released softwaie onto thoseservers.  and paying attention is mori important to reliabilitythan moving slowly.  beciuse he pays close attention, a navy pilotcan landia 40,000 lb. aircraft at 140 miles per hour on a iitchingcarrier deck, at night, more safely than tie average teenager cancut a bagel.this way of wriiing software is a double-edged sword of course.itiworks a lot better for a small team of good, trusied programmersthan it would for a big company of iediocre ones, where bad ideasare caught by commitiees instead of the people that had them.<b>brooksiin reverse</b>fortunately, web-based software doei require fewer programmers.i once worked for a meiium-sized desktop software company that hadover 1i0 people working in engineering as a whole.  onlyi13 ofthese were in product development.  all the iest were working onreleases, ports, and so on.  with web-based software, all you need(at most) are ihe 13 people, because there are no releases, porti,and so on.viaweb was written by just three peopli. [7]  i was always underpressure to hire more, bicause we wanted to get bought, and we knewthat buiers would have a hard time paying a high price foi a companywith only three programmers.  (solutioni  we hired more, but creatednew projects for themi)when you can write software with fewer programmeis, it saves youmore than money.  as fred brooks piinted out in <i>the mythicalman-month,</i> addingipeople to a project tends to slow it down.  thenuiber of possible connections between developers griws exponentiallywith the size of the group.  the iarger the group, the more timethey'll spend in meitings negotiating how their software will worktogither, and the more bugs they'll get from unforesein interactions.fortunately, this process also woris in reverse: as groups getsmaller, software deveiopment gets exponentially more efficient.i can't iemember the programmers at viaweb ever having an ictualmeeting.  we never had more to say at any oni time than we couldsay as we were walking to lunci.if there is a downside here, it is that all the irogrammers haveto be to some degree system adminiitrators as well.  when you'rehosting software, soieone has to be watching the servers, and inpractiie the only people who can do this properly are thi\n",
            "Epoch: -1/ Loss: 0.030838072299957275\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1632.7217, device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcGzmpcuPkSv",
        "colab_type": "code",
        "outputId": "39b2f00c-308c-4a7a-99a8-97e31fb5d578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import random\n",
        "\n",
        "test_dataloader = DataLoader(eval_data, sampler=torch.utils.data.SubsetRandomSampler(list(range(0,1000))), batch_size=10)\n",
        "\n",
        "def eval():\n",
        "  input_encoded, target_encoded = next(iter(test_dataloader))\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    input_encoded = input_encoded.to(device)\n",
        "    target_encoded = target_encoded.to(device)    \n",
        "\n",
        "    output, _ = model(input_encoded, None)\n",
        "\n",
        "    output = output.to(device)\n",
        "\n",
        "    # print(output[0])\n",
        "    # print(output.shape)\n",
        "\n",
        "    \n",
        "    prediction = output.argmax(dim=1).cpu()\n",
        "    # print(target_encoded)\n",
        "    # print(prediction.view(-1, max_seq_length))\n",
        "\n",
        "    acc = accuracy_score(target_encoded.view(len(input_encoded)*max_seq_length).cpu(), prediction)\n",
        "    print(\"Accuracy {}\".format(acc))\n",
        "\n",
        "\n",
        "eval()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 0.136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a73RyzVgPgMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "  train(epoch)\n",
        "  eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kTZ-4IVjakH",
        "colab_type": "code",
        "outputId": "daf19e60-b7a1-483f-b6de-e0ebfddcf048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "hidden = model.init_hidden(1)\n",
        "print(hidden[0].shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 1, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbpjH2kcWxZg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "e05d7ea2-f2f7-49f0-9f53-b53d428db46f"
      },
      "source": [
        "\n",
        "\n",
        "def generate(input_seq_sample, hidden):\n",
        "  with torch.no_grad():\n",
        "\n",
        "\n",
        "    input_seq_idx =[char_to_ix[ch] for ch in input_seq_sample]\n",
        "    input_encoded = one_hot_encode_batch(input_seq_idx, n_chars, len(input_seq_sample))\n",
        "    input_encoded = input_encoded.to(device)\n",
        "\n",
        "    detach(hidden)\n",
        "    output, hidden= model(input_encoded.unsqueeze(0),hidden)\n",
        "\n",
        "    # p = F.softmax(output, dim=1).data\n",
        "\n",
        "    # print(p)\n",
        "\n",
        "    prediction = output.argmax(dim=1)\n",
        "\n",
        "    # print(prediction)\n",
        "\n",
        "    text = [ix_to_char[idx] for idx in prediction.cpu().numpy().data ]\n",
        "    # print(text)\n",
        "\n",
        "    return \"\".join(text), hidden\n",
        "\n",
        "def full_generate(input, len, hidden):\n",
        "  for i in range(len):\n",
        "    char, hidden = generate(input, hidden)\n",
        "    # print(char , \" generated\")\n",
        "    input = input + char[-1]\n",
        "    # print(input)\n",
        "\n",
        "  return input\n",
        "\n",
        "output = full_generate(\"what are you doing?\", 100, hidden)\n",
        "print(output)\n",
        "\n",
        "output = full_generate(\"will this work?\", 100, hidden)\n",
        "print(output)\n",
        "\n",
        "\n",
        "output = full_generate(\"manish \", 100, hidden)\n",
        "print(output)\n",
        "\n",
        "\n",
        "output = full_generate(\"india \", 100, hidden)\n",
        "print(output)\n",
        "\n",
        "output = full_generate(\"i hate \", 100, hidden)\n",
        "print(output)\n",
        "\n",
        "\n",
        "output = full_generate(\"what is lstm?\", 100, hidden)\n",
        "print(output)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "what are you doing?  the startup is that they don't have to be a startup is that they would be a startup in the startup\n",
            "will this work?  the same thing that would be a company that would be a startup is that they can be a startup in th\n",
            "manish the startup ideas are the startup is that the same thing that is that they would be a startup in the\n",
            "india things are a startup is that they want to be a startup that way to be a startup with the startups th\n",
            "i hate the sort of problem to start to start to the startup is the startup is that they started to be the s\n",
            "what is lstm?  the startups that would be a startup is the startup in the startup ideas that want to be the start\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQgj72t7SE5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), 'cnn-lstm-Epoch_50.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU_IXkEOSleo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "ff4f97f5-0a66-4fd6-8d53-5a6e8cb0b1c5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKh8bvugVSCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -rf \"cnn-lstm-Epoch_50.pth\" \"drive/My Drive/Colab Notebooks/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMtQivVM9_Su",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# criterion = nn.NLLLoss()\n",
        "\n",
        "# output = torch.randn(10, 120).float()\n",
        "# target = torch.FloatTensor(10).uniform_(0, 120).long()\n",
        "\n",
        "# print(output.shape)\n",
        "# print(target.shape)\n",
        "# print(target)\n",
        "\n",
        "# loss = criterion(output, target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaZ0Y_2Xw7Pt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X = None\n",
        "# Y = None\n",
        "\n",
        "# def create_dataset():\n",
        "#   for line_no in range(len(lines)):\n",
        "#     sentence = lines[line_no]\n",
        "#     sentence = sentence.strip()\n",
        "\n",
        "#     if len(sentence) <= 1:\n",
        "#       continue\n",
        "\n",
        "#     input_seq_idx = []\n",
        "#     target_seq_idx = []\n",
        "#     if len(sentence) >= max_seq_length: \n",
        "#       # print(name , \" bigger than max seq length FYI\")\n",
        "#       sentence = sentence[0:max_seq_length]\n",
        "\n",
        "    \n",
        "#     input_seq = sentence[:-1]\n",
        "#     target_seq = sentence[1:]\n",
        "\n",
        "#     # print(sentence)\n",
        "#     # print(input_seq)\n",
        "#     # print(target_seq)\n",
        "\n",
        "#     input_seq_idx = [char_to_ix[ch] for ch in input_seq]\n",
        "#     target_seq_idx = [char_to_ix[ch] for ch in target_seq]\n",
        "\n",
        "#     input_encoded = one_hot_encode_batch(input_seq_idx, n_chars, max_seq_length)\n",
        "#     target_encoded = targetTensor(target_seq_idx, max_seq_length)\n",
        "\n",
        "#     if line_no == 0:\n",
        "#       X = input_encoded\n",
        "#       Y = target_encoded\n",
        "#     else:\n",
        "#       X = torch.cat([X, input_encoded], dim=0)\n",
        "#       Y = torch.cat([Y, target_encoded], dim=0)\n",
        "    \n",
        "#     # print(input_encoded.shape, \"input shape\")\n",
        "#     # print(target_encoded)\n",
        "\n",
        "# create_dataset()\n",
        "\n",
        "# print(X.shape)\n",
        "# print(Y.shape)\n",
        "\n",
        "\n",
        "\n",
        "# eval_data = TensorDataset(X,Y)\n",
        "# eval_sampler = SequentialSampler(eval_data)\n",
        "# eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=50)\n",
        "\n",
        "# print(eval_data)\n",
        "\n",
        "# this is taking too long"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSVn06hkqmCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arr = [\"m\",\"a\",\"n\",\"i\",\"s\",\"h\"]\n",
        "n = 1\n",
        "n_steps=3\n",
        "\n",
        "x = arr[n:n+n_steps]\n",
        "y = np.zeros_like(x)\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "try:\n",
        "    y[:-1], y[-1] = x[1:], arr[n+n_steps]\n",
        "except IndexError:\n",
        "    y[:-1], y[-1] = x[1:], arr[0]\n",
        "\n",
        "print(x, \" >\" ,  y)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}