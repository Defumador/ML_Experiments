{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spacy vectors with keras deep nets",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manishiitg/ML_Experments/blob/master/nlp/spacy_vectors_with_keras_deep_nets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eISkkhiuzr1i",
        "colab_type": "text"
      },
      "source": [
        "Trying out spacy word embedding for text classification\n",
        "\n",
        "Run this with GPU else it takes too long to run\n",
        "\n",
        "https://stackoverflow.com/a/56949134  make sure to do this to fix spacy error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViMz2KN20F8l",
        "colab_type": "code",
        "outputId": "f8eb706c-155e-4003-f186-51cc0a4407e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_md==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz#egg=en_core_web_md==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teZ92umcDPXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://nlpforhackers.io/keras-intro/\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import spacy \n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt;\n",
        "from itertools import cycle;\n",
        "import matplotlib.pyplot as plt;\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from collections import defaultdict\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woFJ_aIvTy5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(comment, lowercase=True, remove_stopwords=True):\n",
        "    if lowercase:\n",
        "        comment = comment.lower()\n",
        "    lines = comment.splitlines()\n",
        "    lines = [x.strip(' ') for x in lines]\n",
        "    lines = [x.replace('\"', '') for x in lines]\n",
        "    lines = [x.replace('\\\\\"', '') for x in lines]\n",
        "    lines = [x.replace(u'\\xa0', u'') for x in lines]\n",
        "    comment = \" \".join(lines)\n",
        "    doc = nlp(comment)\n",
        "\n",
        "    # for token in doc:\n",
        "    #   print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "    #     token.shape_, token.is_alpha, token.is_stop)\n",
        "\n",
        "    words = [token for token in doc if token.is_stop !=\n",
        "             True and token.is_punct != True]\n",
        "    # return \" \".join(words)\n",
        "    lemmatized = list()\n",
        "    for word in words:\n",
        "        lemma = word.lemma_.strip()\n",
        "        if lemma:\n",
        "            lemmatized.append(lemma)\n",
        "    return lemmatized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqloCGmfT0pb",
        "colab_type": "code",
        "outputId": "847e7d65-198e-43dc-fdcf-a0855658aea0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "import os\n",
        "\n",
        "def writetofile(dir, filename, data):\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir)\n",
        "    f = os.path.join(dir,str(filename))\n",
        "\n",
        "    with open(f, 'wb') as the_file:\n",
        "      the_file.write(data)\n",
        "\n",
        "\n",
        "categories = [ 'comp.graphics','comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware','comp.sys.mac.hardware','comp.windows.x']\n",
        "news = fetch_20newsgroups(subset=\"train\", categories=categories)\n",
        "\n",
        "\n",
        "clean_data  = []\n",
        "\n",
        "# print(news.keys())\n",
        "\n",
        "# print(news[\"filenames\"][:10])\n",
        "\n",
        "# print(len(news[\"data\"][:1000]))\n",
        "\n",
        "# print(news[\"target_names\"][:50])\n",
        "# print(news[\"target\"][:10])\n",
        "\n",
        "\n",
        "max_limit = 4000\n",
        "\n",
        "targets = news[\"target\"][:max_limit]\n",
        "filenames = news[\"filenames\"][:max_limit]\n",
        "news = news[\"data\"][:max_limit]\n",
        "\n",
        "dir = \"news_group_cleaned\"\n",
        "\n",
        "total_words = 0\n",
        "\n",
        "print(\"cleaning data\")\n",
        "for i, row in enumerate(news):\n",
        "  filename = filenames[i]\n",
        "  filename = filename[(filename.rfind('/'))+1:]\n",
        "  if os.path.exists(filename):\n",
        "    with open(os.path.join(dir, filename), 'r') as content_file:\n",
        "      data = content_file.read()\n",
        "      cleaned = data.split(\" \")\n",
        "      total_words += len(cleaned)\n",
        "  else:\n",
        "    cleaned = normalize(row)\n",
        "    cleaned = \" \".join(cleaned)\n",
        "    writetofile(dir, filename, cleaned.encode(\"utf-8\"))\n",
        "    total_words += len(cleaned)\n",
        "    \n",
        "  \n",
        "  clean_data.append(cleaned)\n",
        "\n",
        "print(\"data cleaned\")\n",
        "\n",
        "print(clean_data[1000])\n",
        "\n",
        "print('total words' , total_words)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cleaning data\n",
            "data cleaned\n",
            "rcaldrn@med.miami.edu richard calderon subject move icon nntp post host epistat.med.miami.edu organization university miami medical school line 17 kmembry@viamar.uucp kirk membry write > remember read program window icon run away > mouse move near know > program ftp location probably cica remember program look call icofrite cica see ago richard calderon rcaldrn@epi.med.miami.edu university miami school medicine information system compute 1029 nw 15 st miami florida 33136\n",
            "total words 3804416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0thUOnIKIuqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# tfid_vectorizer = TfidfVectorizer(max_features=20000)\n",
        "\n",
        "# tfidf = tfid_vectorizer.fit_transform(clean_data)\n",
        "\n",
        "# feature_names = tfid_vectorizer.get_feature_names()\n",
        "\n",
        "# # print(tfid_vectorizer.)\n",
        "\n",
        "# print(len(feature_names))\n",
        "\n",
        "# print(feature_names[2000:3000])\n",
        "\n",
        "# # https://stackoverflow.com/a/35615151\n",
        "\n",
        "# # this is mainly used to extra the max feature nothing else. we can also use count vectorzer for this \n",
        "# # we are not going to use the weights "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fih2mT-sowqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwJ_rz-wS0Pl",
        "colab_type": "code",
        "outputId": "b57c7eb4-187b-4470-c4a5-718d0fe16d16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "source": [
        "#prepare text\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_data)\n",
        "sequences = tokenizer.texts_to_sequences(clean_data)\n",
        "\n",
        "# sequences2 = tokenizer.text_to_word_sequence(clean_data) \n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# print(clean_data[0])\n",
        "# print(data[0])\n",
        "\n",
        "# print(word_index)\n",
        "\n",
        "labels = to_categorical(np.asarray(targets))\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# split the data into a training set and a validation set\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
        "\n",
        "x_train = data[:-nb_validation_samples]\n",
        "y_train = labels[:-nb_validation_samples]\n",
        "x_val = data[-nb_validation_samples:]\n",
        "y_val = labels[-nb_validation_samples:]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found 64456 unique tokens.\n",
            "Shape of data tensor: (2936, 1000)\n",
            "Shape of label tensor: (2936, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KEHleYqG9Uz",
        "colab_type": "code",
        "outputId": "c292ba11-73f9-4606-e609-7a63ffbba18c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "EMBEDDINGS_LEN = len(nlp.vocab['apple'].vector)\n",
        "print(EMBEDDINGS_LEN)\n",
        "\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDINGS_LEN))\n",
        "for word, idx in word_index.items():\n",
        "    try:\n",
        "        embedding = nlp.vocab[word].vector\n",
        "        embedding_matrix[idx] = embedding\n",
        "    except:\n",
        "      pass"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EciNXLQ_j4yU",
        "colab_type": "code",
        "outputId": "ea5a2729-6267-4530-d70d-fe7ccc882b1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(len(word_index) + 1,\n",
        "                            EMBEDDINGS_LEN,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dense(units=len(categories), activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
        "          epochs=12, batch_size=128,shuffle=True)\n",
        "\n",
        "# scores = model.evaluate(X_test, Y_test, verbose=1)\n",
        "\n",
        "# print(\"Accuracy: \", scores[1])\n",
        "# print(\"Loss: \", scores[0])\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 1000, 300)         19337100  \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 996, 128)          192128    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 199, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 195, 128)          82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 39, 128)           0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 4992)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               639104    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 20,251,025\n",
            "Trainable params: 913,925\n",
            "Non-trainable params: 19,337,100\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 2349 samples, validate on 587 samples\n",
            "Epoch 1/12\n",
            "2349/2349 [==============================] - 1s 611us/step - loss: 1.6205 - acc: 0.2312 - val_loss: 1.5878 - val_acc: 0.2930\n",
            "Epoch 2/12\n",
            "2349/2349 [==============================] - 1s 221us/step - loss: 1.4167 - acc: 0.4125 - val_loss: 1.2061 - val_acc: 0.4991\n",
            "Epoch 3/12\n",
            "2349/2349 [==============================] - 1s 220us/step - loss: 1.0616 - acc: 0.5526 - val_loss: 1.0490 - val_acc: 0.5656\n",
            "Epoch 4/12\n",
            "2349/2349 [==============================] - 1s 217us/step - loss: 0.7467 - acc: 0.7263 - val_loss: 0.8196 - val_acc: 0.7036\n",
            "Epoch 5/12\n",
            "2349/2349 [==============================] - 1s 216us/step - loss: 0.4983 - acc: 0.8382 - val_loss: 0.8039 - val_acc: 0.7206\n",
            "Epoch 6/12\n",
            "2349/2349 [==============================] - 1s 221us/step - loss: 0.3115 - acc: 0.9127 - val_loss: 0.6957 - val_acc: 0.7479\n",
            "Epoch 7/12\n",
            "2349/2349 [==============================] - 1s 216us/step - loss: 0.1829 - acc: 0.9600 - val_loss: 0.6547 - val_acc: 0.7734\n",
            "Epoch 8/12\n",
            "2349/2349 [==============================] - 1s 217us/step - loss: 0.1064 - acc: 0.9860 - val_loss: 0.6445 - val_acc: 0.7836\n",
            "Epoch 9/12\n",
            "2349/2349 [==============================] - 1s 219us/step - loss: 0.0532 - acc: 0.9953 - val_loss: 0.7778 - val_acc: 0.7751\n",
            "Epoch 10/12\n",
            "2349/2349 [==============================] - 1s 219us/step - loss: 0.0326 - acc: 0.9970 - val_loss: 0.7420 - val_acc: 0.7836\n",
            "Epoch 11/12\n",
            "2349/2349 [==============================] - 1s 217us/step - loss: 0.0217 - acc: 0.9979 - val_loss: 0.7003 - val_acc: 0.7956\n",
            "Epoch 12/12\n",
            "2349/2349 [==============================] - 1s 214us/step - loss: 0.0094 - acc: 0.9996 - val_loss: 0.7319 - val_acc: 0.7888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7faf6b39add8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZHXtWsarE6o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "outputId": "3e221b3d-2abd-416e-ff85-0937b81d78c7"
      },
      "source": [
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten\n",
        "from keras.layers import LSTM\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(len(word_index) + 1,\n",
        "                            EMBEDDINGS_LEN,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dense(units=len(categories), activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
        "          epochs=12, batch_size=32,shuffle=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 1000, 300)         19337100  \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 996, 128)          192128    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 199, 128)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 19,677,969\n",
            "Trainable params: 340,869\n",
            "Non-trainable params: 19,337,100\n",
            "_________________________________________________________________\n",
            "Train on 2349 samples, validate on 587 samples\n",
            "Epoch 1/12\n",
            "2349/2349 [==============================] - 29s 12ms/step - loss: 1.3731 - acc: 0.3810 - val_loss: 1.0423 - val_acc: 0.5622\n",
            "Epoch 2/12\n",
            "2349/2349 [==============================] - 28s 12ms/step - loss: 0.8945 - acc: 0.6505 - val_loss: 0.8795 - val_acc: 0.6627\n",
            "Epoch 3/12\n",
            "2349/2349 [==============================] - 28s 12ms/step - loss: 0.6129 - acc: 0.7748 - val_loss: 0.6600 - val_acc: 0.7479\n",
            "Epoch 4/12\n",
            "2349/2349 [==============================] - 29s 12ms/step - loss: 0.4625 - acc: 0.8442 - val_loss: 0.7213 - val_acc: 0.7138\n",
            "Epoch 5/12\n",
            "2349/2349 [==============================] - 29s 12ms/step - loss: 0.3442 - acc: 0.8795 - val_loss: 0.5848 - val_acc: 0.7905\n",
            "Epoch 6/12\n",
            "2349/2349 [==============================] - 28s 12ms/step - loss: 0.2788 - acc: 0.9059 - val_loss: 0.5164 - val_acc: 0.8262\n",
            "Epoch 7/12\n",
            "2349/2349 [==============================] - 28s 12ms/step - loss: 0.1999 - acc: 0.9353 - val_loss: 0.5702 - val_acc: 0.8245\n",
            "Epoch 8/12\n",
            "2349/2349 [==============================] - 28s 12ms/step - loss: 0.1735 - acc: 0.9404 - val_loss: 0.6991 - val_acc: 0.7871\n",
            "Epoch 9/12\n",
            "2349/2349 [==============================] - 28s 12ms/step - loss: 0.1132 - acc: 0.9647 - val_loss: 0.5795 - val_acc: 0.8399\n",
            "Epoch 10/12\n",
            "2349/2349 [==============================] - 28s 12ms/step - loss: 0.1123 - acc: 0.9681 - val_loss: 0.7851 - val_acc: 0.7530\n",
            "Epoch 11/12\n",
            "2349/2349 [==============================] - 28s 12ms/step - loss: 0.1084 - acc: 0.9630 - val_loss: 0.6486 - val_acc: 0.8313\n",
            "Epoch 12/12\n",
            "2349/2349 [==============================] - 28s 12ms/step - loss: 0.0396 - acc: 0.9889 - val_loss: 0.7464 - val_acc: 0.8177\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7faf16f02400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    }
  ]
}