{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch cnn experiment basics",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manishiitg/ML_Experiments/blob/master/pytorch/101/pytorch_cnn_experiment_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jAbo7-bFdae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://blog.algorithmia.com/convolutional-neural-nets-in-pytorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvNetCustomTest(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "      super(ConvNetCustomTest, self).__init__()\n",
        "\n",
        "      #Input channels = 3, output channels = 18\n",
        "      self.conv1 = torch.nn.Conv2d(3, 18, kernel_size=3, stride=1, padding=1)\n",
        "      self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "      \n",
        "      #4608 input features, 64 output features (see sizing flow below)\n",
        "      self.fc1 = torch.nn.Linear(18 * 16 * 16, 64)\n",
        "        \n",
        "      #64 input features, 10 output features for our 10 defined classes\n",
        "      self.fc2 = torch.nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self,x) :\n",
        "      x = F.relu(self.conv1(x))\n",
        "      #Size changes from (3, 32, 32) to (18, 32, 32)\n",
        "\n",
        "      x = self.pool(x)\n",
        "      #Size changes from (18, 32, 32) to (18, 16, 16)\n",
        "\n",
        "      #Reshape data to input to the input layer of the neural net\n",
        "      #Size changes from (18, 16, 16) to (1, 4608)\n",
        "      #Recall that the -1 infers this dimension from the other given dimension\n",
        "      x = x.view(-1, 18 * 16 *16)\n",
        "\n",
        "      x = F.relu(self.fc1(x))\n",
        "\n",
        "      x = self.fc2(x)\n",
        "      return(x)\n",
        "      \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF-tnwq4JrIW",
        "colab_type": "code",
        "outputId": "6b93f04c-880e-4d63-9565-a76190c3950d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root='./cifardata', train=True, download=True, transform=transform)\n",
        "\n",
        "test_set = torchvision.datasets.CIFAR10(root='./cifardata', train=False, download=True, transform=transform)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifardata/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:07, 24296055.90it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./cifardata/cifar-10-python.tar.gz to ./cifardata\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8_aBT0tKLDW",
        "colab_type": "code",
        "outputId": "ba0c2b5f-702f-4706-9bb0-63704d46573c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(train_set[0][0].shape)\n",
        "\n",
        "print(len(train_set))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 32, 32])\n",
            "50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH5SseDNUS92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "train_dataset, val_dataset, test_dataset, _ = random_split(train_set, [800, 150, 50, 49000])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLcvcf5xZk5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=50, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(train_set, batch_size=50, num_workers=2)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usFOYoakbFEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def createLossAndOptimizer(net, learning_rate=0.001):\n",
        "    \n",
        "    #Loss function\n",
        "    loss = torch.nn.CrossEntropyLoss()\n",
        "    \n",
        "    #Optimizer\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    \n",
        "    return(loss, optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFghxYb1bdML",
        "colab_type": "code",
        "outputId": "ac6db3bf-bcdd-405c-c6c3-b0084f0bdccb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "import time\n",
        "\n",
        "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
        "    \n",
        "    #Print all of the hyperparameters of the training iteration:\n",
        "    print(\"===== HYPERPARAMETERS =====\")\n",
        "    print(\"batch_size=\", batch_size)\n",
        "    print(\"epochs=\", n_epochs)\n",
        "    print(\"learning_rate=\", learning_rate)\n",
        "    print(\"=\" * 30)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,num_workers=2)\n",
        "\n",
        "    #Get training data\n",
        "    # train_loader = get_train_loader(batch_size)\n",
        "    n_batches = len(train_loader)\n",
        "    \n",
        "    #Create our loss and optimizer functions\n",
        "    loss, optimizer = createLossAndOptimizer(net, learning_rate)\n",
        "    \n",
        "    #Time for printing\n",
        "    training_start_time = time.time()\n",
        "    \n",
        "    #Loop for n_epochs\n",
        "    for epoch in range(n_epochs):\n",
        "        \n",
        "        running_loss = 0.0\n",
        "        print_every = n_batches // 10\n",
        "        start_time = time.time()\n",
        "        total_train_loss = 0\n",
        "        \n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "            # print(data[0])\n",
        "            #Get inputs\n",
        "            inputs, labels = data\n",
        "            \n",
        "            #Wrap them in a Variable object\n",
        "            # inputs, labels = Variable(inputs), Variable(labels)\n",
        "            \n",
        "            #Set the parameter gradients to zero\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            #Forward pass, backward pass, optimize\n",
        "            outputs = net(inputs)\n",
        "            loss_size = loss(outputs, labels)\n",
        "            loss_size.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            #Print statistics\n",
        "\n",
        "            running_loss += loss_size.item()\n",
        "            total_train_loss += loss_size.item()\n",
        "            \n",
        "            #Print every 10th batch of an epoch\n",
        "            if (i + 1) % (print_every + 1) == 0:\n",
        "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
        "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
        "                #Reset running loss and time\n",
        "                running_loss = 0.0\n",
        "                start_time = time.time()\n",
        "            \n",
        "        #At the end of the epoch, do a pass on the validation set\n",
        "        total_val_loss = 0\n",
        "        for inputs, labels in val_loader:\n",
        "                        \n",
        "            #Forward pass\n",
        "            val_outputs = net(inputs)\n",
        "            val_loss_size = loss(val_outputs, labels)\n",
        "            total_val_loss += val_loss_size.item()\n",
        "            \n",
        "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(val_loader)))\n",
        "        \n",
        "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))\n",
        "\n",
        "CNN = ConvNetCustomTest()\n",
        "trainNet(CNN, batch_size=32, n_epochs=5, learning_rate=0.001)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== HYPERPARAMETERS =====\n",
            "batch_size= 32\n",
            "epochs= 5\n",
            "learning_rate= 0.001\n",
            "==============================\n",
            "Epoch 1, 12% \t train_loss: 3.39 took: 0.53s\n",
            "Epoch 1, 24% \t train_loss: 3.46 took: 0.06s\n",
            "Epoch 1, 36% \t train_loss: 3.34 took: 0.07s\n",
            "Epoch 1, 48% \t train_loss: 3.31 took: 0.07s\n",
            "Epoch 1, 60% \t train_loss: 3.24 took: 0.08s\n",
            "Epoch 1, 72% \t train_loss: 3.11 took: 0.07s\n",
            "Epoch 1, 84% \t train_loss: 3.29 took: 0.07s\n",
            "Epoch 1, 96% \t train_loss: 3.20 took: 0.05s\n",
            "Validation loss = 2.08\n",
            "Epoch 2, 12% \t train_loss: 2.88 took: 0.14s\n",
            "Epoch 2, 24% \t train_loss: 3.09 took: 0.06s\n",
            "Epoch 2, 36% \t train_loss: 2.91 took: 0.07s\n",
            "Epoch 2, 48% \t train_loss: 3.04 took: 0.07s\n",
            "Epoch 2, 60% \t train_loss: 2.90 took: 0.07s\n",
            "Epoch 2, 72% \t train_loss: 2.81 took: 0.07s\n",
            "Epoch 2, 84% \t train_loss: 3.08 took: 0.07s\n",
            "Epoch 2, 96% \t train_loss: 3.05 took: 0.06s\n",
            "Validation loss = 1.98\n",
            "Epoch 3, 12% \t train_loss: 2.62 took: 0.14s\n",
            "Epoch 3, 24% \t train_loss: 3.08 took: 0.07s\n",
            "Epoch 3, 36% \t train_loss: 2.58 took: 0.06s\n",
            "Epoch 3, 48% \t train_loss: 2.73 took: 0.06s\n",
            "Epoch 3, 60% \t train_loss: 2.62 took: 0.07s\n",
            "Epoch 3, 72% \t train_loss: 2.56 took: 0.07s\n",
            "Epoch 3, 84% \t train_loss: 2.85 took: 0.06s\n",
            "Epoch 3, 96% \t train_loss: 2.89 took: 0.05s\n",
            "Validation loss = 1.93\n",
            "Epoch 4, 12% \t train_loss: 2.34 took: 0.14s\n",
            "Epoch 4, 24% \t train_loss: 2.92 took: 0.07s\n",
            "Epoch 4, 36% \t train_loss: 2.32 took: 0.07s\n",
            "Epoch 4, 48% \t train_loss: 2.52 took: 0.07s\n",
            "Epoch 4, 60% \t train_loss: 2.40 took: 0.07s\n",
            "Epoch 4, 72% \t train_loss: 2.34 took: 0.07s\n",
            "Epoch 4, 84% \t train_loss: 2.63 took: 0.07s\n",
            "Epoch 4, 96% \t train_loss: 2.72 took: 0.06s\n",
            "Validation loss = 1.89\n",
            "Epoch 5, 12% \t train_loss: 2.12 took: 0.14s\n",
            "Epoch 5, 24% \t train_loss: 2.77 took: 0.07s\n",
            "Epoch 5, 36% \t train_loss: 2.16 took: 0.07s\n",
            "Epoch 5, 48% \t train_loss: 2.36 took: 0.07s\n",
            "Epoch 5, 60% \t train_loss: 2.19 took: 0.07s\n",
            "Epoch 5, 72% \t train_loss: 2.13 took: 0.07s\n",
            "Epoch 5, 84% \t train_loss: 2.44 took: 0.07s\n",
            "Epoch 5, 96% \t train_loss: 2.55 took: 0.06s\n",
            "Validation loss = 1.86\n",
            "Training finished, took 94.58s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTwlUYOlexNP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7e75b5ba-e5e6-4250-99dc-585d5387046f"
      },
      "source": [
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images\n",
        "        labels = labels\n",
        "        outputs = CNN(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 34.47 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}