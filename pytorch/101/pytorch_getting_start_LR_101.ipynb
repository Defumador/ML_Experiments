{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch getting start LR 101",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manishiitg/ML_Experiments/blob/master/pytorch/101/pytorch_getting_start_LR_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FYDfPPpK-5S",
        "colab_type": "text"
      },
      "source": [
        "# This code is to learn basic of pytoch and getting started with ML/Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ52HwjeHk1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e\n",
        "\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odiU8XhzLjhy",
        "colab_type": "text"
      },
      "source": [
        "Setting up some random data to solve a linear regression problem "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpbictYiGnZq",
        "colab_type": "code",
        "outputId": "6f18a4ca-b006-473e-8d7b-3e09ab2c3aaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "\n",
        "np.random.seed(69)\n",
        "\n",
        "x = np.random.rand(100, 1)\n",
        "y = 1 + 2 * x + .1 * np.random.randn(100, 1)\n",
        "\n",
        "# Shuffles the indices\n",
        "idx = np.arange(100)\n",
        "np.random.shuffle(idx)\n",
        "\n",
        "# Uses first 80 random indices for train\n",
        "train_idx = idx[:80]\n",
        "# Uses the remaining indices for validation\n",
        "val_idx = idx[80:]\n",
        "\n",
        "# Generates train and validation sets\n",
        "x_train, y_train = x[train_idx], y[train_idx]\n",
        "x_val, y_val = x[val_idx], y[val_idx]\n",
        "\n",
        "plt.plot(x_train, y_train, 'o', color='red')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fb86e838978>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD1CAYAAABA+A6aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBnQiIj/BgE5E5Cf+HyQj7/QSpqf0AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2EdDJZ9IJpR",
        "colab_type": "code",
        "outputId": "876a1498-f764-4dc8-96bd-21f69d071a2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plt.plot(x_val, y_val, 'o', color='blue')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fb86e38f748>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD1CAYAAABA+A6aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBAudiEgi/g+YSVKkYDYG9QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-KVy1cgL7ZT",
        "colab_type": "text"
      },
      "source": [
        "Solving LR with simply numpy array and math"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDDM0Ky-IU8G",
        "colab_type": "code",
        "outputId": "441f7f8f-a0bb-4d10-a4a3-205e420d20f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "np.random.seed(69)\n",
        "\n",
        "a = np.random.randn(1)\n",
        "b = np.random.randn(1)\n",
        "\n",
        "print(a, b)\n",
        "# Sets learning rate\n",
        "lr = 1e-1\n",
        "# Defines number of epochs\n",
        "n_epochs = 1000\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Computes our model's predicted output\n",
        "    yhat = a + b * x_train\n",
        "    \n",
        "    # How wrong is our model? That's the error! \n",
        "    error = (y_train - yhat)\n",
        "    # It is a regression, so it computes mean squared error (MSE)\n",
        "    loss = (error ** 2).mean()\n",
        "\n",
        "    # Computes gradients for both \"a\" and \"b\" parameters\n",
        "    a_grad = -2 * error.mean()\n",
        "    b_grad = -2 * (x_train * error).mean()\n",
        "\n",
        "    # Updates parameters using gradients and the learning rate\n",
        "    a = a - lr * a_grad\n",
        "    b = b - lr * b_grad\n",
        "\n",
        "print(a, b)\n",
        "\n",
        "\n",
        "# Sanity Check: do we get the same results as our gradient descent?\n",
        "from sklearn.linear_model import LinearRegression\n",
        "linr = LinearRegression()\n",
        "linr.fit(x_train, y_train)\n",
        "print(linr.intercept_, linr.coef_[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.9155071] [-0.60354197]\n",
            "[1.00907371] [2.00651272]\n",
            "[1.00906796] [2.00652444]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuKdofMmON1-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "2306476b-ef8d-4a24-b8f3-1c3c5a06d7fd"
      },
      "source": [
        "!pip install torchviz"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n",
            "\r\u001b[K     |████████                        | 10kB 18.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.3.0+cu100)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.17.3)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.1-cp36-none-any.whl size=3520 sha256=9a824201876bae20c8ed6b925a09105f727ea936b9117fca1b9cdae9d471c104\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHeEot40KrEa",
        "colab_type": "code",
        "outputId": "da9e264e-13ad-4d29-c505-0764ebb5cdd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torchviz import make_dot\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
        "# and then we send them to the chosen device\n",
        "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
        "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
        "\n",
        "# Here we can see the difference - notice that .type() is more useful\n",
        "# since it also tells us WHERE the tensor is (device)\n",
        "print(type(x_train), type(x_train_tensor), x_train_tensor.type())\n",
        "\n",
        "torch.manual_seed(69)\n",
        "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "print(\"initial values of a,b\")\n",
        "print(a, b)\n",
        "\n",
        "n_epochs = 100\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    yhat = a + b * x_train_tensor\n",
        "    error = y_train_tensor - yhat\n",
        "    loss = (error ** 2).mean()\n",
        "\n",
        "    print(\"gradient before: \", a.grad,b.grad)\n",
        "\n",
        "    # We just tell PyTorch to work its way BACKWARDS from the specified loss!\n",
        "    # backward() function automatically calculates gradients for all variables which has requires_grad=True set\n",
        "    # loss is a variable which is associated to a,b using the loss we defined above\n",
        "    # loss => error => yhat => a,b\n",
        "    loss.backward()\n",
        "    # Let's check the computed gradients...\n",
        "    print(\"gradient after:\", a.grad,b.grad)\n",
        "\n",
        "    with torch.no_grad():  # doing this i.e no_grad because we don't want to pytorch to store the gradients again \n",
        "      a -= lr * a.grad\n",
        "      b -= lr * b.grad\n",
        "      # we update the variables \n",
        "\n",
        "    a.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "    # setting gradients to zero again\n",
        "    \n",
        "print(\"final values\", a, b)\n",
        "\n",
        "# this is one of the most critical and fundamental packages of pytorch https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html \n",
        "# called \"autograd\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n",
            "<class 'numpy.ndarray'> <class 'torch.Tensor'> torch.FloatTensor\n",
            "initial values of a,b\n",
            "tensor([-0.5259], requires_grad=True) tensor([-2.6043], requires_grad=True)\n",
            "gradient before:  None None\n",
            "gradient after: tensor([-7.3181]) tensor([-4.0500])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([-5.4814]) tensor([-3.1442])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([-4.0954]) tensor([-2.4594])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([-3.0497]) tensor([-1.9415])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([-2.2609]) tensor([-1.5495])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([-1.6659]) tensor([-1.2527])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([-1.2173]) tensor([-1.0276])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([-0.8792]) tensor([-0.8567])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([-0.6244]) tensor([-0.7267])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([-0.4326]) tensor([-0.6276])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([-0.2882]) tensor([-0.5519])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([-0.1797]) tensor([-0.4938])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([-0.0983]) tensor([-0.4490])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([-0.0373]) tensor([-0.4143])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0083]) tensor([-0.3871])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0423]) tensor([-0.3658])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0676]) tensor([-0.3488])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0862]) tensor([-0.3351])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0998]) tensor([-0.3238])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1097]) tensor([-0.3145])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1167]) tensor([-0.3067])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1216]) tensor([-0.2999])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1249]) tensor([-0.2940])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1270]) tensor([-0.2887])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1282]) tensor([-0.2839])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1287]) tensor([-0.2795])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1287]) tensor([-0.2753])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1284]) tensor([-0.2715])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1277]) tensor([-0.2678])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1268]) tensor([-0.2642])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1258]) tensor([-0.2608])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1247]) tensor([-0.2575])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1235]) tensor([-0.2543])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1222]) tensor([-0.2511])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1209]) tensor([-0.2480])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1196]) tensor([-0.2450])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1182]) tensor([-0.2420])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1169]) tensor([-0.2390])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1155]) tensor([-0.2361])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1142]) tensor([-0.2333])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1128]) tensor([-0.2305])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1115]) tensor([-0.2277])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1102]) tensor([-0.2250])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1089]) tensor([-0.2222])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1076]) tensor([-0.2196])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1063]) tensor([-0.2169])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1050]) tensor([-0.2143])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1038]) tensor([-0.2117])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1025]) tensor([-0.2092])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1013]) tensor([-0.2067])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.1001]) tensor([-0.2042])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0989]) tensor([-0.2018])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0977]) tensor([-0.1993])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0965]) tensor([-0.1969])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0954]) tensor([-0.1946])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0942]) tensor([-0.1922])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0931]) tensor([-0.1899])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0920]) tensor([-0.1876])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0909]) tensor([-0.1854])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0898]) tensor([-0.1832])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0887]) tensor([-0.1810])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0876]) tensor([-0.1788])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0866]) tensor([-0.1766])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0855]) tensor([-0.1745])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0845]) tensor([-0.1724])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0835]) tensor([-0.1704])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0825]) tensor([-0.1683])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0815]) tensor([-0.1663])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0805]) tensor([-0.1643])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0796]) tensor([-0.1623])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0786]) tensor([-0.1604])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0777]) tensor([-0.1584])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0767]) tensor([-0.1565])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0758]) tensor([-0.1547])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0749]) tensor([-0.1528])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0740]) tensor([-0.1510])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0731]) tensor([-0.1492])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0722]) tensor([-0.1474])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0714]) tensor([-0.1456])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0705]) tensor([-0.1439])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0697]) tensor([-0.1421])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0688]) tensor([-0.1404])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0680]) tensor([-0.1387])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0672]) tensor([-0.1371])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0664]) tensor([-0.1354])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0656]) tensor([-0.1338])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0648]) tensor([-0.1322])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0640]) tensor([-0.1306])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0632]) tensor([-0.1290])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0625]) tensor([-0.1275])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0617]) tensor([-0.1260])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0610]) tensor([-0.1244])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0603]) tensor([-0.1229])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0595]) tensor([-0.1215])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0588]) tensor([-0.1200])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0581]) tensor([-0.1186])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0574]) tensor([-0.1171])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0567]) tensor([-0.1157])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0560]) tensor([-0.1143])\n",
            "gradient before:  tensor([0.]) tensor([0.])\n",
            "gradient after: tensor([0.0554]) tensor([-0.1130])\n",
            "final values tensor([1.4646], requires_grad=True) tensor([1.0770], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngEdpx-VP-d3",
        "colab_type": "text"
      },
      "source": [
        "Pytorch Dynamic Computations Graphs\n",
        "\n",
        "https://ai.stackexchange.com/questions/3801/what-is-a-dynamic-computational-graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A23GnvHvM4K7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "\n",
        "yhat = a + b * x_train_tensor\n",
        "error = y_train_tensor - yhat\n",
        "loss = (error ** 2).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vArhzmMPNNnk",
        "colab_type": "code",
        "outputId": "dda79ae2-49db-4505-a31d-6c2b6a49969b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "make_dot(yhat) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fe6e9616f28>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"172pt\" height=\"171pt\"\n viewBox=\"0.00 0.00 171.50 171.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 167)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-167 167.5,-167 167.5,4 -4,4\"/>\n<!-- 140629734678992 -->\n<g id=\"node1\" class=\"node\">\n<title>140629734678992</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"118,-21 26,-21 26,0 118,0 118,-21\"/>\n<text text-anchor=\"middle\" x=\"72\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n</g>\n<!-- 140629734679272 -->\n<g id=\"node2\" class=\"node\">\n<title>140629734679272</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-92 0,-92 0,-57 54,-57 54,-92\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 140629734679272&#45;&gt;140629734678992 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140629734679272&#45;&gt;140629734678992</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M39.535,-56.6724C45.4798,-48.2176 52.5878,-38.1085 58.6352,-29.5078\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"61.5714,-31.4169 64.4601,-21.2234 55.8452,-27.3906 61.5714,-31.4169\"/>\n</g>\n<!-- 140629734679384 -->\n<g id=\"node3\" class=\"node\">\n<title>140629734679384</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-85 72.5,-85 72.5,-64 163.5,-64 163.5,-85\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-71.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140629734679384&#45;&gt;140629734678992 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140629734679384&#45;&gt;140629734678992</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M110.404,-63.9317C103.7191,-54.6309 93.821,-40.8597 85.7479,-29.6276\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"88.4395,-27.3753 79.761,-21.2979 82.7553,-31.4608 88.4395,-27.3753\"/>\n</g>\n<!-- 140629734679216 -->\n<g id=\"node4\" class=\"node\">\n<title>140629734679216</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"145,-163 91,-163 91,-128 145,-128 145,-163\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-135.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 140629734679216&#45;&gt;140629734679384 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140629734679216&#45;&gt;140629734679384</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118,-127.9494C118,-118.058 118,-105.6435 118,-95.2693\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-95.0288 118,-85.0288 114.5001,-95.0289 121.5001,-95.0288\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7sQqlwuNPMF",
        "colab_type": "code",
        "outputId": "8a91cd9a-46c2-4cc6-82ee-3edb5bdee265",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        }
      },
      "source": [
        "make_dot(error) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fe6e9616198>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"172pt\" height=\"228pt\"\n viewBox=\"0.00 0.00 171.50 228.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 224)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-224 167.5,-224 167.5,4 -4,4\"/>\n<!-- 140629734678600 -->\n<g id=\"node1\" class=\"node\">\n<title>140629734678600</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"117,-21 27,-21 27,0 117,0 117,-21\"/>\n<text text-anchor=\"middle\" x=\"72\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SubBackward0</text>\n</g>\n<!-- 140629734678992 -->\n<g id=\"node2\" class=\"node\">\n<title>140629734678992</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"118,-78 26,-78 26,-57 118,-57 118,-78\"/>\n<text text-anchor=\"middle\" x=\"72\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n</g>\n<!-- 140629734678992&#45;&gt;140629734678600 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140629734678992&#45;&gt;140629734678600</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M72,-56.7787C72,-49.6134 72,-39.9517 72,-31.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"75.5001,-31.1732 72,-21.1732 68.5001,-31.1732 75.5001,-31.1732\"/>\n</g>\n<!-- 140629734679272 -->\n<g id=\"node3\" class=\"node\">\n<title>140629734679272</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-149 0,-149 0,-114 54,-114 54,-149\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 140629734679272&#45;&gt;140629734678992 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140629734679272&#45;&gt;140629734678992</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M39.535,-113.6724C45.4798,-105.2176 52.5878,-95.1085 58.6352,-86.5078\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"61.5714,-88.4169 64.4601,-78.2234 55.8452,-84.3906 61.5714,-88.4169\"/>\n</g>\n<!-- 140629734679384 -->\n<g id=\"node4\" class=\"node\">\n<title>140629734679384</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-142 72.5,-142 72.5,-121 163.5,-121 163.5,-142\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-128.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140629734679384&#45;&gt;140629734678992 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140629734679384&#45;&gt;140629734678992</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M110.404,-120.9317C103.7191,-111.6309 93.821,-97.8597 85.7479,-86.6276\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"88.4395,-84.3753 79.761,-78.2979 82.7553,-88.4608 88.4395,-84.3753\"/>\n</g>\n<!-- 140629734679216 -->\n<g id=\"node5\" class=\"node\">\n<title>140629734679216</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"145,-220 91,-220 91,-185 145,-185 145,-220\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-192.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 140629734679216&#45;&gt;140629734679384 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140629734679216&#45;&gt;140629734679384</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118,-184.9494C118,-175.058 118,-162.6435 118,-152.2693\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-152.0288 118,-142.0288 114.5001,-152.0289 121.5001,-152.0288\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d2tk8HNNTAP",
        "colab_type": "code",
        "outputId": "6e478d82-d035-4b51-f71f-dad35aa507aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        }
      },
      "source": [
        "make_dot(loss) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fe6e960b6d8>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"172pt\" height=\"342pt\"\n viewBox=\"0.00 0.00 171.50 342.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 338)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-338 167.5,-338 167.5,4 -4,4\"/>\n<!-- 140629734634216 -->\n<g id=\"node1\" class=\"node\">\n<title>140629734634216</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"121,-21 23,-21 23,0 121,0 121,-21\"/>\n<text text-anchor=\"middle\" x=\"72\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MeanBackward0</text>\n</g>\n<!-- 140629734635168 -->\n<g id=\"node2\" class=\"node\">\n<title>140629734635168</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"118.5,-78 25.5,-78 25.5,-57 118.5,-57 118.5,-78\"/>\n<text text-anchor=\"middle\" x=\"72\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">PowBackward0</text>\n</g>\n<!-- 140629734635168&#45;&gt;140629734634216 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140629734635168&#45;&gt;140629734634216</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M72,-56.7787C72,-49.6134 72,-39.9517 72,-31.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"75.5001,-31.1732 72,-21.1732 68.5001,-31.1732 75.5001,-31.1732\"/>\n</g>\n<!-- 140629734635560 -->\n<g id=\"node3\" class=\"node\">\n<title>140629734635560</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"117,-135 27,-135 27,-114 117,-114 117,-135\"/>\n<text text-anchor=\"middle\" x=\"72\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SubBackward0</text>\n</g>\n<!-- 140629734635560&#45;&gt;140629734635168 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140629734635560&#45;&gt;140629734635168</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M72,-113.7787C72,-106.6134 72,-96.9517 72,-88.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"75.5001,-88.1732 72,-78.1732 68.5001,-88.1732 75.5001,-88.1732\"/>\n</g>\n<!-- 140629734635448 -->\n<g id=\"node4\" class=\"node\">\n<title>140629734635448</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"118,-192 26,-192 26,-171 118,-171 118,-192\"/>\n<text text-anchor=\"middle\" x=\"72\" y=\"-178.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n</g>\n<!-- 140629734635448&#45;&gt;140629734635560 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140629734635448&#45;&gt;140629734635560</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M72,-170.7787C72,-163.6134 72,-153.9517 72,-145.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"75.5001,-145.1732 72,-135.1732 68.5001,-145.1732 75.5001,-145.1732\"/>\n</g>\n<!-- 140629734634104 -->\n<g id=\"node5\" class=\"node\">\n<title>140629734634104</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-263 0,-263 0,-228 54,-228 54,-263\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-235.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 140629734634104&#45;&gt;140629734635448 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140629734634104&#45;&gt;140629734635448</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M39.535,-227.6724C45.4798,-219.2176 52.5878,-209.1085 58.6352,-200.5078\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"61.5714,-202.4169 64.4601,-192.2234 55.8452,-198.3906 61.5714,-202.4169\"/>\n</g>\n<!-- 140629734634944 -->\n<g id=\"node6\" class=\"node\">\n<title>140629734634944</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-256 72.5,-256 72.5,-235 163.5,-235 163.5,-256\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-242.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140629734634944&#45;&gt;140629734635448 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140629734634944&#45;&gt;140629734635448</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M110.404,-234.9317C103.7191,-225.6309 93.821,-211.8597 85.7479,-200.6276\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"88.4395,-198.3753 79.761,-192.2979 82.7553,-202.4608 88.4395,-198.3753\"/>\n</g>\n<!-- 140629734636064 -->\n<g id=\"node7\" class=\"node\">\n<title>140629734636064</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"145,-334 91,-334 91,-299 145,-299 145,-334\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-306.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 140629734636064&#45;&gt;140629734634944 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140629734636064&#45;&gt;140629734634944</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118,-298.9494C118,-289.058 118,-276.6435 118,-266.2693\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-266.0288 118,-256.0288 114.5001,-266.0289 121.5001,-266.0288\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebhydRPPQpdI",
        "colab_type": "text"
      },
      "source": [
        "LR => Optmizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1-VmFlEOKek",
        "colab_type": "code",
        "outputId": "4a349502-68c0-42cf-af18-b38cb979ed38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "print(a, b)\n",
        "\n",
        "lr = 1e-1\n",
        "n_epochs = 1000\n",
        "\n",
        "# Defines a SGD optimizer to update the parameters\n",
        "optimizer = optim.SGD([a, b], lr=lr)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    yhat = a + b * x_train_tensor\n",
        "    error = y_train_tensor - yhat\n",
        "    loss = (error ** 2).mean()\n",
        "\n",
        "    loss.backward()    \n",
        "    \n",
        "    # No more manual update!\n",
        "    # with torch.no_grad():\n",
        "    #     a -= lr * a.grad\n",
        "    #     b -= lr * b.grad\n",
        "    optimizer.step() \n",
        "    \n",
        "    # No more telling PyTorch to let gradients go!\n",
        "    # a.grad.zero_()\n",
        "    # b.grad.zero_()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "print(a, b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n",
            "tensor([1.0091], requires_grad=True) tensor([2.0065], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2QGDvY6Q3m6",
        "colab_type": "text"
      },
      "source": [
        "LR => Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izpvkGz_OyGP",
        "colab_type": "code",
        "outputId": "0c5fdb61-6714-4e6f-a36e-1f037b30230d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "print(a, b)\n",
        "\n",
        "lr = 1e-1\n",
        "n_epochs = 1000\n",
        "\n",
        "# Defines a MSE loss function\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "optimizer = optim.SGD([a, b], lr=lr)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    yhat = a + b * x_train_tensor\n",
        "    \n",
        "    # No more manual loss!\n",
        "    # error = y_tensor - yhat\n",
        "    # loss = (error ** 2).mean()\n",
        "    loss = loss_fn(y_train_tensor, yhat)\n",
        "\n",
        "    loss.backward()    \n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "print(a, b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n",
            "tensor([1.0091], requires_grad=True) tensor([2.0065], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSf335lDRr1d",
        "colab_type": "text"
      },
      "source": [
        "At this stage we have covered the very basics of pytorch and also solved LR problem using it "
      ]
    }
  ]
}