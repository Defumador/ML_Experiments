{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep learning word2vec model with CNN, LSTM",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manishiitg/ML_Experments/blob/master/keras/deep_learning_word2vec_model_with_CNN%2C_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OUsxPExs094",
        "colab_type": "text"
      },
      "source": [
        "Trying out CNN and LSTM models for text classification. \n",
        "\n",
        "Previously i tried out simple FF network, but the results where not very good. So exprimenting with CNN/LSTM models "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8BgySsfAZAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import spacy \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from sklearn.cluster import KMeans;\n",
        "from sklearn.neighbors import KDTree;\n",
        "\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt;\n",
        "from itertools import cycle;\n",
        "import matplotlib.pyplot as plt;\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "from collections import defaultdict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6QoH3KgAk6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(comment, lowercase=True, remove_stopwords=True):\n",
        "    if lowercase:\n",
        "        comment = comment.lower()\n",
        "    lines = comment.splitlines()\n",
        "    lines = [x.strip(' ') for x in lines]\n",
        "    lines = [x.replace('\"', '') for x in lines]\n",
        "    lines = [x.replace('\\\\\"', '') for x in lines]\n",
        "    lines = [x.replace(u'\\xa0', u'') for x in lines]\n",
        "    comment = \" \".join(lines)\n",
        "    doc = nlp(comment)\n",
        "\n",
        "    # for token in doc:\n",
        "    #   print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "    #     token.shape_, token.is_alpha, token.is_stop)\n",
        "\n",
        "    words = [token for token in doc if token.is_stop !=\n",
        "             True and token.is_punct != True]\n",
        "    # return \" \".join(words)\n",
        "    lemmatized = list()\n",
        "    for word in words:\n",
        "        lemma = word.lemma_.strip()\n",
        "        if lemma:\n",
        "            lemmatized.append(lemma)\n",
        "    return lemmatized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D7oPvDpBKdH",
        "colab_type": "code",
        "outputId": "e05a504d-6ec7-488b-a65a-7df541ff570f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os\n",
        "\n",
        "def writetofile(dir, filename, data):\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir)\n",
        "    f = os.path.join(dir,str(filename))\n",
        "\n",
        "    with open(f, 'wb') as the_file:\n",
        "      the_file.write(data)\n",
        "\n",
        "\n",
        "categories = [ 'comp.graphics','comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware','comp.sys.mac.hardware','comp.windows.x']\n",
        "news = fetch_20newsgroups(subset=\"train\", categories=categories)\n",
        "\n",
        "\n",
        "clean_data  = []\n",
        "\n",
        "# print(news.keys())\n",
        "\n",
        "# print(news[\"filenames\"][:10])\n",
        "\n",
        "# print(len(news[\"data\"][:1000]))\n",
        "\n",
        "# print(news[\"target_names\"][:50])\n",
        "# print(news[\"target\"][:10])\n",
        "\n",
        "\n",
        "max_limit = 4000\n",
        "\n",
        "targets = news[\"target\"][:max_limit]\n",
        "filenames = news[\"filenames\"][:max_limit]\n",
        "news = news[\"data\"][:max_limit]\n",
        "\n",
        "dir = \"news_group_cleaned\"\n",
        "\n",
        "print(\"cleaning data\")\n",
        "for i, row in enumerate(news):\n",
        "  filename = filenames[i]\n",
        "  filename = filename[(filename.rfind('/'))+1:]\n",
        "  if os.path.exists(filename):\n",
        "    with open(os.path.join(dir, filename), 'r') as content_file:\n",
        "      data = content_file.read()\n",
        "      cleaned = data.split(\" \")\n",
        "  else:\n",
        "    cleaned = normalize(row)\n",
        "    writetofile(dir, filename, \" \".join(cleaned).encode(\"utf-8\"))\n",
        "    \n",
        "  \n",
        "  clean_data.append(cleaned)\n",
        "\n",
        "print(\"data cleaned\")\n",
        "\n",
        "# print(clean_data[10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cleaning data\n",
            "data cleaned\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEBkC2Z4BNQT",
        "colab_type": "code",
        "outputId": "341b76fd-e0f1-4264-95e7-b95b5de53635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "source": [
        "import gensim, logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "model = gensim.models.Word2Vec(clean_data)\n",
        "\n",
        "\n",
        "# model = KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec')\n",
        "\n",
        "#https://stackoverflow.com/questions/45159693/word2vec-models-consist-of-characters-instead-of-words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 20:32:13,548 : INFO : collecting all words and their counts\n",
            "2019-10-14 20:32:13,550 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2019-10-14 20:32:13,718 : INFO : collected 66979 word types from a corpus of 470869 raw words and 2936 sentences\n",
            "2019-10-14 20:32:13,719 : INFO : Loading a fresh vocabulary\n",
            "2019-10-14 20:32:13,777 : INFO : effective_min_count=5 retains 7758 unique words (11% of original 66979, drops 59221)\n",
            "2019-10-14 20:32:13,778 : INFO : effective_min_count=5 leaves 392121 word corpus (83% of original 470869, drops 78748)\n",
            "2019-10-14 20:32:13,814 : INFO : deleting the raw counts dictionary of 66979 items\n",
            "2019-10-14 20:32:13,817 : INFO : sample=0.001 downsamples 33 most-common words\n",
            "2019-10-14 20:32:13,818 : INFO : downsampling leaves estimated 326228 word corpus (83.2% of prior 392121)\n",
            "2019-10-14 20:32:13,853 : INFO : estimated required memory for 7758 words and 100 dimensions: 10085400 bytes\n",
            "2019-10-14 20:32:13,855 : INFO : resetting layer weights\n",
            "2019-10-14 20:32:17,013 : INFO : training model with 3 workers on 7758 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2019-10-14 20:32:17,683 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-10-14 20:32:17,692 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-10-14 20:32:17,714 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-10-14 20:32:17,716 : INFO : EPOCH - 1 : training on 470869 raw words (326470 effective words) took 0.7s, 469008 effective words/s\n",
            "2019-10-14 20:32:18,501 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-10-14 20:32:18,509 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-10-14 20:32:18,532 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-10-14 20:32:18,534 : INFO : EPOCH - 2 : training on 470869 raw words (325944 effective words) took 0.8s, 402558 effective words/s\n",
            "2019-10-14 20:32:19,416 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-10-14 20:32:19,420 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-10-14 20:32:19,433 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-10-14 20:32:19,435 : INFO : EPOCH - 3 : training on 470869 raw words (326298 effective words) took 0.9s, 365279 effective words/s\n",
            "2019-10-14 20:32:20,338 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-10-14 20:32:20,347 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-10-14 20:32:20,365 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-10-14 20:32:20,367 : INFO : EPOCH - 4 : training on 470869 raw words (326368 effective words) took 0.9s, 354284 effective words/s\n",
            "2019-10-14 20:32:21,126 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-10-14 20:32:21,144 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-10-14 20:32:21,163 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-10-14 20:32:21,165 : INFO : EPOCH - 5 : training on 470869 raw words (326182 effective words) took 0.8s, 413490 effective words/s\n",
            "2019-10-14 20:32:21,166 : INFO : training on a 2354345 raw words (1631262 effective words) took 4.2s, 392908 effective words/s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0UtFbqxCeF-",
        "colab_type": "code",
        "outputId": "f509b44d-673b-4fbd-81a2-650113679cbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        }
      },
      "source": [
        "#prepare text\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_data)\n",
        "sequences = tokenizer.texts_to_sequences(clean_data)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "labels = to_categorical(np.asarray(targets))\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# split the data into a training set and a validation set\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
        "\n",
        "x_train = data[:-nb_validation_samples]\n",
        "y_train = labels[:-nb_validation_samples]\n",
        "x_val = data[-nb_validation_samples:]\n",
        "y_val = labels[-nb_validation_samples:]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/mnt/c/work/testflask/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/mnt/c/work/testflask/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/mnt/c/work/testflask/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/mnt/c/work/testflask/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/mnt/c/work/testflask/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/mnt/c/work/testflask/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/mnt/c/work/testflask/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/mnt/c/work/testflask/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/mnt/c/work/testflask/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/mnt/c/work/testflask/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/mnt/c/work/testflask/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/mnt/c/work/testflask/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 66979 unique tokens.\n",
            "Shape of data tensor: (2936, 1000)\n",
            "Shape of label tensor: (2936, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJdWUQEJBjgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIM = 100\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "\n",
        "# this is index of words which we tokenized i.e a map with word ==> index\n",
        "\n",
        "for word, i in word_index.items():\n",
        "  if word in model.wv.vocab:\n",
        "    # print(\"found word\" , word)\n",
        "    embedding_vector = model.wv[word]\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "  # else:\n",
        "  #   print(\"work not found\" , word)\n",
        "  # if i> 100:\n",
        "  #   break\n",
        "  # else:\n",
        "  #   print(\"word not found\", word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9uFljTnBSjs",
        "colab_type": "code",
        "outputId": "1ac05939-5754-4a53-bd38-685b96c92e1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "\n",
        "\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dense(units=len(categories), activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
        "          epochs=12, batch_size=128,shuffle=True)\n",
        "\n",
        "# scores = model.evaluate(X_test, Y_test, verbose=1)\n",
        "\n",
        "# print(\"Accuracy: \", scores[1])\n",
        "# print(\"Loss: \", scores[0])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_15 (Embedding)     (None, 1000, 100)         6698000   \n",
            "_________________________________________________________________\n",
            "conv1d_16 (Conv1D)           (None, 996, 128)          64128     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_16 (MaxPooling (None, 199, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_17 (Conv1D)           (None, 195, 128)          82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_17 (MaxPooling (None, 39, 128)           0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 4992)              0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 128)               639104    \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 7,483,925\n",
            "Trainable params: 785,925\n",
            "Non-trainable params: 6,698,000\n",
            "_________________________________________________________________\n",
            "Train on 2349 samples, validate on 587 samples\n",
            "Epoch 1/12\n",
            "1792/2349 [=====================>........] - ETA: 5s - loss: 1.4805 - accuracy: 0.3320"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEmBTEvG0bnt",
        "colab_type": "code",
        "outputId": "54d05865-0a2b-494a-f33b-9358023ba70e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        }
      },
      "source": [
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten\n",
        "from keras.layers import LSTM\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dense(units=len(categories), activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
        "          epochs=12, batch_size=32,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_14 (Embedding)     (None, 1000, 100)         6698000   \n",
            "_________________________________________________________________\n",
            "conv1d_15 (Conv1D)           (None, 996, 128)          64128     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_15 (MaxPooling (None, 199, 128)          0         \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 6,910,869\n",
            "Trainable params: 212,869\n",
            "Non-trainable params: 6,698,000\n",
            "_________________________________________________________________\n",
            "Train on 2349 samples, validate on 587 samples\n",
            "Epoch 1/12\n",
            "2349/2349 [==============================] - 50s 21ms/step - loss: 1.3660 - accuracy: 0.3725 - val_loss: 1.1786 - val_accuracy: 0.5009\n",
            "Epoch 2/12\n",
            "2349/2349 [==============================] - 42s 18ms/step - loss: 1.1900 - accuracy: 0.4619 - val_loss: 1.0924 - val_accuracy: 0.5094\n",
            "Epoch 3/12\n",
            "2349/2349 [==============================] - 49s 21ms/step - loss: 1.1101 - accuracy: 0.5057 - val_loss: 1.0243 - val_accuracy: 0.5707\n",
            "Epoch 4/12\n",
            "2349/2349 [==============================] - 47s 20ms/step - loss: 1.0198 - accuracy: 0.5628 - val_loss: 0.9467 - val_accuracy: 0.6082\n",
            "Epoch 5/12\n",
            "2349/2349 [==============================] - 53s 23ms/step - loss: 0.9442 - accuracy: 0.6092 - val_loss: 0.9134 - val_accuracy: 0.6440\n",
            "Epoch 6/12\n",
            "2349/2349 [==============================] - 42s 18ms/step - loss: 0.8961 - accuracy: 0.6305 - val_loss: 0.8559 - val_accuracy: 0.6644\n",
            "Epoch 7/12\n",
            "2349/2349 [==============================] - 48s 20ms/step - loss: 0.8439 - accuracy: 0.6471 - val_loss: 0.9342 - val_accuracy: 0.6422\n",
            "Epoch 8/12\n",
            "2349/2349 [==============================] - 45s 19ms/step - loss: 0.7994 - accuracy: 0.6769 - val_loss: 0.8284 - val_accuracy: 0.6780\n",
            "Epoch 9/12\n",
            "2349/2349 [==============================] - 44s 19ms/step - loss: 0.7349 - accuracy: 0.7092 - val_loss: 0.8564 - val_accuracy: 0.6661\n",
            "Epoch 10/12\n",
            "2349/2349 [==============================] - 42s 18ms/step - loss: 0.7228 - accuracy: 0.7203 - val_loss: 0.8338 - val_accuracy: 0.6763\n",
            "Epoch 11/12\n",
            "2349/2349 [==============================] - 43s 18ms/step - loss: 0.6537 - accuracy: 0.7327 - val_loss: 0.9070 - val_accuracy: 0.6593\n",
            "Epoch 12/12\n",
            "2349/2349 [==============================] - 43s 18ms/step - loss: 0.6343 - accuracy: 0.7403 - val_loss: 0.7933 - val_accuracy: 0.6934\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fd0d5c64278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}