{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple FF deep learning classifier with word2vec embedding + TF-IDF",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manishiitg/ML_Experments/blob/master/keras/simple_FF_deep_learning_classifier_with_word2vec_embedding_%2B_TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBCIDqLwe4h0",
        "colab_type": "text"
      },
      "source": [
        "NLP Simple Text Classification with Keras!\n",
        "\n",
        "This is a very simple model using word2vec embeddings + TF-IDF\n",
        "\n",
        "This is just to experiment and see how this performs and as expected the results are low.\n",
        "\n",
        "Need to look at LSTM or CNN models for better learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xYPlNR7kwWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import spacy \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from sklearn.cluster import KMeans;\n",
        "from sklearn.neighbors import KDTree;\n",
        "\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt;\n",
        "from itertools import cycle;\n",
        "import matplotlib.pyplot as plt;\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "from collections import defaultdict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KybA57_ak1FA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(comment, lowercase=True, remove_stopwords=True):\n",
        "    if lowercase:\n",
        "        comment = comment.lower()\n",
        "    lines = comment.splitlines()\n",
        "    lines = [x.strip(' ') for x in lines]\n",
        "    lines = [x.replace('\"', '') for x in lines]\n",
        "    lines = [x.replace('\\\\\"', '') for x in lines]\n",
        "    lines = [x.replace(u'\\xa0', u'') for x in lines]\n",
        "    comment = \" \".join(lines)\n",
        "    doc = nlp(comment)\n",
        "\n",
        "    # for token in doc:\n",
        "    #   print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "    #     token.shape_, token.is_alpha, token.is_stop)\n",
        "\n",
        "    words = [token for token in doc if token.is_stop !=\n",
        "             True and token.is_punct != True]\n",
        "    # return \" \".join(words)\n",
        "    lemmatized = list()\n",
        "    for word in words:\n",
        "        lemma = word.lemma_.strip()\n",
        "        if lemma:\n",
        "            lemmatized.append(lemma)\n",
        "    return lemmatized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB9ELUOBk3Nz",
        "colab_type": "code",
        "outputId": "3c53aad4-458f-4a25-b405-701a7feec424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "import os\n",
        "\n",
        "def writetofile(dir, filename, data):\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir)\n",
        "    f = os.path.join(dir,str(filename))\n",
        "\n",
        "    with open(f, 'wb') as the_file:\n",
        "      the_file.write(data)\n",
        "\n",
        "\n",
        "categories = [ 'comp.graphics','comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware','comp.sys.mac.hardware','comp.windows.x']\n",
        "news = fetch_20newsgroups(subset=\"train\", categories=categories)\n",
        "\n",
        "\n",
        "clean_data  = []\n",
        "\n",
        "# print(news.keys())\n",
        "\n",
        "# print(news[\"filenames\"][:10])\n",
        "\n",
        "# print(len(news[\"data\"][:1000]))\n",
        "\n",
        "# print(news[\"target_names\"][:50])\n",
        "# print(news[\"target\"][:10])\n",
        "\n",
        "\n",
        "max_limit = 4000\n",
        "\n",
        "targets = news[\"target\"][:max_limit]\n",
        "filenames = news[\"filenames\"][:max_limit]\n",
        "news = news[\"data\"][:max_limit]\n",
        "\n",
        "dir = \"news_group_cleaned\"\n",
        "\n",
        "print(\"cleaning data\")\n",
        "for i, row in enumerate(news):\n",
        "  filename = filenames[i]\n",
        "  filename = filename[(filename.rfind('/'))+1:]\n",
        "  if os.path.exists(filename):\n",
        "    with open(os.path.join(dir, filename), 'r') as content_file:\n",
        "      data = content_file.read()\n",
        "      cleaned = data.split(\" \")\n",
        "  else:\n",
        "    cleaned = normalize(row)\n",
        "    writetofile(dir, filename, \" \".join(cleaned).encode(\"utf-8\"))\n",
        "    \n",
        "  \n",
        "  clean_data.append(cleaned)\n",
        "\n",
        "print(\"data cleaned\")\n",
        "\n",
        "# print(clean_data[10])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cleaning data\n",
            "data cleaned\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sahiqu1mk4lM",
        "colab_type": "code",
        "outputId": "4e511e58-da32-4a4a-c657-0d3f5bd323ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "source": [
        "import gensim, logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "model = gensim.models.Word2Vec(clean_data)\n",
        "\n",
        "#https://stackoverflow.com/questions/45159693/word2vec-models-consist-of-characters-instead-of-words"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-11-08 05:32:56,344 : INFO : collecting all words and their counts\n",
            "2019-11-08 05:32:56,345 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2019-11-08 05:32:56,463 : INFO : collected 66979 word types from a corpus of 470869 raw words and 2936 sentences\n",
            "2019-11-08 05:32:56,464 : INFO : Loading a fresh vocabulary\n",
            "2019-11-08 05:32:56,503 : INFO : effective_min_count=5 retains 7758 unique words (11% of original 66979, drops 59221)\n",
            "2019-11-08 05:32:56,504 : INFO : effective_min_count=5 leaves 392121 word corpus (83% of original 470869, drops 78748)\n",
            "2019-11-08 05:32:56,535 : INFO : deleting the raw counts dictionary of 66979 items\n",
            "2019-11-08 05:32:56,539 : INFO : sample=0.001 downsamples 33 most-common words\n",
            "2019-11-08 05:32:56,540 : INFO : downsampling leaves estimated 326228 word corpus (83.2% of prior 392121)\n",
            "2019-11-08 05:32:56,571 : INFO : estimated required memory for 7758 words and 100 dimensions: 10085400 bytes\n",
            "2019-11-08 05:32:56,572 : INFO : resetting layer weights\n",
            "2019-11-08 05:32:58,129 : INFO : training model with 3 workers on 7758 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2019-11-08 05:32:58,745 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-08 05:32:58,771 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-08 05:32:58,774 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-08 05:32:58,775 : INFO : EPOCH - 1 : training on 470869 raw words (326092 effective words) took 0.6s, 510561 effective words/s\n",
            "2019-11-08 05:32:59,419 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-08 05:32:59,427 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-08 05:32:59,437 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-08 05:32:59,439 : INFO : EPOCH - 2 : training on 470869 raw words (326319 effective words) took 0.7s, 494629 effective words/s\n",
            "2019-11-08 05:33:00,071 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-08 05:33:00,080 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-08 05:33:00,096 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-08 05:33:00,098 : INFO : EPOCH - 3 : training on 470869 raw words (326113 effective words) took 0.7s, 500198 effective words/s\n",
            "2019-11-08 05:33:00,811 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-08 05:33:00,841 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-08 05:33:00,843 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-08 05:33:00,847 : INFO : EPOCH - 4 : training on 470869 raw words (326322 effective words) took 0.7s, 440619 effective words/s\n",
            "2019-11-08 05:33:01,518 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-08 05:33:01,525 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-08 05:33:01,540 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-08 05:33:01,541 : INFO : EPOCH - 5 : training on 470869 raw words (326203 effective words) took 0.7s, 481220 effective words/s\n",
            "2019-11-08 05:33:01,542 : INFO : training on a 2354345 raw words (1631049 effective words) took 3.4s, 478099 effective words/s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DOhYuc4k54f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TfidfEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        self.word2weight = None\n",
        "        self.dim = 100\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
        "        tfidf.fit(X)\n",
        "        # if a word was never seen - it must be at least as infrequent\n",
        "        # as any of the known words - so the default idf is the max of \n",
        "        # known idf's\n",
        "        max_idf = max(tfidf.idf_)\n",
        "        #https://stackoverflow.com/a/5900634\n",
        "        self.word2weight = defaultdict(\n",
        "            lambda: max_idf,\n",
        "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
        "                         for w in words if w in self.word2vec] or\n",
        "                        [np.zeros(self.dim)], axis=0)\n",
        "                for words in X\n",
        "            ])\n",
        "\n",
        "w2v = dict(zip(model.wv.index2word, model.wv.vectors))\n",
        "\n",
        "tf = TfidfEmbeddingVectorizer(w2v)\n",
        "tf.fit(clean_data, targets)\n",
        "embeddings = tf.transform(clean_data)   \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAUL7Oauk7EJ",
        "colab_type": "code",
        "outputId": "bd4eb339-d9cb-48b4-b7b4-a59d7151efcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(len(embeddings))\n",
        "print(len(targets))\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(embeddings, targets, test_size=0.33, random_state=42)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2936\n",
            "2936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8riGVzUlBAk",
        "colab_type": "code",
        "outputId": "a6ac8b00-35a9-430b-de3e-776aa769858a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(units=len(X_test), activation='relu', input_dim=100))\n",
        "model.add(Dense(units=5, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.fit(X_train, Y_train,\n",
        "          epochs=5, batch_size=128, verbose=1, validation_split=.2)\n",
        "\n",
        "\n",
        "scores = model.evaluate(X_test, Y_test, verbose=1)\n",
        "\n",
        "\n",
        "print(\"Accuracy: \", scores[1])\n",
        "print(\"Loss: \", scores[0])\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-08 05:33:48,324 : WARNING : From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "2019-11-08 05:33:48,335 : WARNING : From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "2019-11-08 05:33:48,343 : WARNING : From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "2019-11-08 05:33:48,377 : WARNING : From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "2019-11-08 05:33:48,400 : WARNING : From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "2019-11-08 05:33:48,550 : WARNING : From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "2019-11-08 05:33:48,585 : WARNING : From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 969)               97869     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 4850      \n",
            "=================================================================\n",
            "Total params: 102,719\n",
            "Trainable params: 102,719\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-08 05:33:48,659 : WARNING : From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "2019-11-08 05:33:48,701 : WARNING : From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-11-08 05:33:48,709 : WARNING : From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "2019-11-08 05:33:48,710 : WARNING : From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "2019-11-08 05:33:48,744 : WARNING : From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "2019-11-08 05:33:48,745 : WARNING : From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1573 samples, validate on 394 samples\n",
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-08 05:33:48,960 : WARNING : From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1573/1573 [==============================] - 1s 664us/step - loss: 1.4017 - acc: 0.3821 - val_loss: 1.2129 - val_acc: 0.4518\n",
            "Epoch 2/5\n",
            "1573/1573 [==============================] - 0s 114us/step - loss: 1.1571 - acc: 0.4660 - val_loss: 1.1336 - val_acc: 0.4822\n",
            "Epoch 3/5\n",
            "1573/1573 [==============================] - 0s 121us/step - loss: 1.1061 - acc: 0.5270 - val_loss: 1.1412 - val_acc: 0.4746\n",
            "Epoch 4/5\n",
            "1573/1573 [==============================] - 0s 91us/step - loss: 1.0935 - acc: 0.5308 - val_loss: 1.0620 - val_acc: 0.5305\n",
            "Epoch 5/5\n",
            "1573/1573 [==============================] - 0s 82us/step - loss: 1.0361 - acc: 0.5709 - val_loss: 1.0349 - val_acc: 0.5584\n",
            "969/969 [==============================] - 0s 39us/step\n",
            "Accuracy:  0.5356037154163247\n",
            "Loss:  1.0854043298707534\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HacotYfVA0EV",
        "colab_type": "text"
      },
      "source": [
        "As we see the score is quite low. simple NN are not suitable for text based neural nets, we will look at CNN or LSTM i.e RNN"
      ]
    }
  ]
}